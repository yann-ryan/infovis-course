[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Information Visualization and the Humanities",
    "section": "",
    "text": "Course Overview\nWelcome! You’re probably here because you are taking the course Information Visualization and the Humanities, taking place in Semester 1, 2023-2024, at Leiden University. It’s part of the minor in Digital Humanities. The course instructor is Dr. Yann Ryan.\nThis e-book will serve as the course website. The menu on the left side lists the chapters, one for each week of the course. More will appear as the course progresses. The menu on the right contains the table of contents for this particular chapter, so you can skip to different sections.\nYou’ll also need to familiarise yourself with the course Brightspace area, where I’ll put further information and you’ll submit assignments."
  },
  {
    "objectID": "index.html#what-do-we-mean-by-information-visualisation",
    "href": "index.html#what-do-we-mean-by-information-visualisation",
    "title": "Information Visualization and the Humanities",
    "section": "What do we mean by ‘information visualisation’?",
    "text": "What do we mean by ‘information visualisation’?\nInformation visualization (or data visualization, which I’ll use interchangeablely) could be defined as the graphical representation of information and data.\nData visualization is often used to convey or communicate messages about complex or large-scale data. It has been around in its present form—charts, graphs and so forth—since at least the eighteenth century. One form of data visualisation—maps—have been around for even longer. Today, we come across data visualizations in our daily lives: in everything from scientific articles to news media, and in advertisements.\nIn the humanities, the data we use might be, for example, historical statistics or networks, or the text (or data derived from it) found in literary works. It might be something totally different such as audio, images, or spatial data."
  },
  {
    "objectID": "index.html#course-aims-and-objectives",
    "href": "index.html#course-aims-and-objectives",
    "title": "Information Visualization and the Humanities",
    "section": "Course Aims and Objectives",
    "text": "Course Aims and Objectives\n\nBe able to articulate how to leverage info visualization to provide an edge in a range of situations, including academic research as well as non-academic professions.\nUnderstand how a FAIR (Findable, Accessible, Interoperable, and Reusable) approach to data is important for your work and that of others.\nBe able to critically reflect on a range of visualizations, with a specific focus on network and spatial data.\nKnow where to find some of the inspiring visualizations, thinkers, and designers in this field.\nBe able to discuss your views on information visualization with peers as well as give and receive feedback.\nBe able to use a number of information visualization tools, such as network analytic and GIS software or interactive visualization tools.\n\nIn terms of concrete skills, by the end of this course, you should be able to use humanities-focused data to make informative, highly-customised visualisations such as this:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggthemes)\n\noptions(scipen = 999999)\n\ndf = data.table::fread('box_office.csv')\n\ndf = df %&gt;% \n  mutate(billions =total_inflation_adjusted_box_office/1000000000 )\n\nggplot() + \n  geom_point(data = df, aes(year, as.numeric(billions)), size= 1, alpha = .5) +\n  geom_smooth(data = df, aes(year, as.numeric(billions)), method = \"lm\",\n              formula = y ~ poly(x, 23), se = FALSE, color = 'black', alpha = .75) + \n  scale_y_continuous(limits = c(0, 15)) + \n  labs(title = 'Domestic Yearly Box Office, 1995 to 2023', \n       subtitle = \"In billion US dollars, adjusted for inflation\", \n       x = NULL, \n       y = \"Box office in billions US (inflation adjusted)\")  +\n  geom_vline(xintercept=2020, color = 'red', alpha = .6, size = 2) +\n  annotate(\"text\", x=2019, y=5, label=\"First Pandemic Year\", angle=90) + \n  theme_economist()\n\n\n\n\n\nor this:\n\n\nShow the code\nlibrary(ggrepel)\n\ndf2 = data.table::fread(\"top_movie.csv\")\n\ndf2 = df2 %&gt;%\n  mutate(billions =total_in_2022_dollars/1000)\n\ntop_data = df2 %&gt;% \n  slice_max(order_by = total_in_2022_dollars, n = 5)\n\n   p =  ggplot() + \n      geom_point(data = df2,(aes(x = year, \n                                 y = billions, \n                                 size = billions, \n                                 fill = distributor)), \n                 pch = 21, color = 'black', alpha = .6, stroke= .5) + \n      geom_label_repel(data = top_data, aes(x = year, \n                                            y = billions, \n                                            label = movie, \n                                            fill = distributor), \n                       alpha = .6,show.legend=FALSE) + \n      scale_size_area(max_size = 6) + \n      labs(title = \"The Rise and Rise of Disney\", \n           subtitle = \"Nearly 1/3 of the highest-grossing movies (in the US) of the last 30 years\\nhave been made by the Disney Company\") + \n      theme_fivethirtyeight() + \n      labs(y = \"Box office in billions US (inflation adjusted)\", \n           size = \"Takings (Billions): \", fill = 'Distributor: ') + \n      theme(legend.position = 'bottom', \n            legend.box=\"vertical\", \n            legend.margin=margin(),\n            plot.title = element_text(face = 'bold', size  = 15))\n    \np\n\n\n\n\n\n\n\nShow the code\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\n\nstar_wars_edges = read_csv('StarWars_Social_Network/json_to_csv/starwars-episode-4-interactions.edges.csv')\n\nstar_wars_nodes = read_csv('StarWars_Social_Network/json_to_csv/starwars-episode-4-interactions.nodes.csv')\n\ng = star_wars_edges %&gt;% \n  as_tbl_graph() %&gt;% \n  mutate(degree = centrality_degree()) %&gt;% \n  left_join(star_wars_nodes %&gt;% mutate(Id = as.character(Id)), by = c('name' = 'Id')) %&gt;% \n  mutate(name.y = snakecase::to_title_case(name.y))\n\ng %&gt;% ggraph(layout = 'linear', circular = TRUE) + \n  geom_node_point() + \n  geom_node_text(aes(label = name.y, angle = node_angle(x, y)), hjust = -0.5, size = 3) + \n  geom_edge_arc(aes(alpha = value, color = value))+\n  coord_fixed(xlim = c(-1.4, 1.4), ylim = c(-1.4, 1.4)) +\n  guides(fill = guide_none()) +\n  theme_fivethirtyeight() + \n  theme(legend.position = 'none',\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())  + \n  scale_edge_color_viridis(direction = -1) + \n  labs(title = \"Star Wars Social Network\", \n       subtitle = \"Episode IV: A New Hope\")\n\n\n\n\n\nPerhaps most importantly, you will have total control over every aspect of the visualisation, from the colours, to the titles, labels, and scales. You’ll even be able to publish them in a smart-looking report, which will look a bit like this page.\n\nInformation Visualisation…\nMost importantly, you’ll learn the basic principles of information visualization, and put them into practice using a platform called R (more on that below). R is widely used by, for example, data journalists at publications like The Economist and the Financial Times.\nThe visualisations above were made with a few lines of the R language. If you click the ‘Show the code’ button above them, you’ll see the code I used to create them. It might look a little complicated now, but if you follow the course, you should be able to make similar (and hopefully much nicer) ones by the end of the twelve weeks.\nIn your final project, you’ll combine text, data and visual outputs to make a report on a topic you’re interested in. You’ll make a document like this. For this course, it’s more than enough to stick to the basic charts like the above. But if you’re interested, you can explore fancier things such as interactive maps and charts, 3D-rendered visualisations, and more.\n\n\n…and the Humanities\nThis is not to say the course will be an exhaustive ‘how to’ for scientific data visualisations. One of your superpowers as a humanities student is that you learn to to move beyond the surface level and consider the subtexts, biases, and so forth in cultural objects. In this course, we’ll put those skills to good use, and you’ll use your own expertise and interests to tell compelling and truthful stories about humanities subjects, using data.\nJust as importantly, you’ll learn how to critique data visualisations and design. As a group we’ll ask: what makes them good or bad, scientific or unethical? Can we trust the intentions behind commercial or political data visualisations? Can we spot when they are being misleading, and describe exactly how? What are the negative and positive connotations around certain graphical representations, and why do we use them? How can we make better, more ethical data visualisations? How can we make visualisations which properly consider those with accessibility needs?"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Information Visualization and the Humanities",
    "section": "Schedule",
    "text": "Schedule\nBelow is the course schedule. In the first couple of weeks, you’ll learn the basics of the R platform, and the practical steps of how to use it for data wrangling and to produce visualisations. We’ll first work with ‘dummy’ data, while you think about how you can use your own interests and expertise, and find the data you’d like to use for the rest of the course.\nAfter this, we’ll do things in a slightly unconventional order. Week 4 is a ‘field trip’ to the Library, where you’ll learn about maps from a traditional, humanities perspective from the curator of maps, Martijn Storms.\nSo we can use and think about the themes of Martijn’s lecture, after this we’ll jump straight into a section on digital mapping. You’ll learn how to create your own maps from scratch, and how to critique existing ones and those of your peers. Through maps, we’ll learn some more about key information visualisation concepts, such colour theory.\nAfter this, we’ll go back to learning about conventional data visualisations, followed by a section on networks. In the final weeks of the course, we’ll learn about putting the finishing touches on your work, talk about the final project, and there will be some space for you to do some experiments in some additional areas of interest.\n\nCourse Schedule\n\n\n\n\n\n\n\nWeek\nDate\nCourse topic\n\n\n\n\n1\n2023-09-11\nIntroduction\n\nIntroduction to and history of data visualisation.\nSigning up to Posit Cloud\nCreating a Markdown document\n\n\n\n2\n2023-09-18\nIntroduction to R\n\nData wrangling.\nFAIR data principles\n\n\n\n3\n2023-09-25\nData Visualisation 0\n\nData visualisation principles\nMaking data visualisations with R\n\n\n\n4\n2023-10-09\nData visualisation 1:\n\nBasic principles\nChart suitability for different data types\nColor theory and types\n\n\n\n5\n2023-10-16\nData visualisation 2\n\nAdvanced chart types\nEthics and accessibility\n\n\n\n6\n2023-10-23\nField trip to the library\n\n\n7\n2023-11-06\nDigital mapping 1\n\nThe sf package\nCreating a ‘base map’ with rnaturalearth\nMapping data points\n\n\n\n8\n2023-11-13\nDigital mapping 2\n\nChoropleth (thematic) maps\nUsing Open Street Map\n\n\n\n9\n2023-11-20\nNetwork visualisation 1:\n\nNetwork theory and data types\nBasic network visualisations\n\n\n\n10\n2023-11-27\nNetwork visualisation 2\n\nAdvanced network visualisations\n\n\n\n11\n2023-12-04\nFinishing touches\n\nAesthetics and design choices\nAnnotations, titles\nColour and pre-attentive features\n\n\n\n12\n2023-12-11\nFinal class\n\nFinal project discussion\nIndividual interests"
  },
  {
    "objectID": "index.html#data-visualisation",
    "href": "index.html#data-visualisation",
    "title": "Information Visualization and the Humanities",
    "section": "Data Visualisation",
    "text": "Data Visualisation\nIn practice, creating data visualisations involves mapping numerical data of some sort to graphical representations - often called geometries. A geometry might be the length of a line, the x and y position on a graph, or the temperature of a colour on a map. A good data visualisation will use this mapping to tell a story about the data, while representing it faithfully.\nYou’ll put these skills into practice using a tool called R. R is a coding language often used for data science - it’s particularly good at working with what are called ‘dataframes’: structured data in the form of rows and columns which we’ll mostly be using on this course.\nTo work with R, we have set up a learning environment called ‘Posit cloud’. Later, you’ll be given instructions on how to sign up and access your workspace.\nEach assignment (including the final project) will be submitted in the form of a report. This is a html document (a web page) containing visualisations and text.\nUsing a coding language like this allows us to have direct control over every aspect of a visualisation - we can choose from many different geometries depending on our data and what we want to show, adjust the colours in precise ways, and pick the scale we want to use. We can also use R to summarise data, or limit it to just the parts we want to work with. This will complement the learning of Python you’re doing in your other course.\nIn this course, you’ll learn these very practical skills. It might be hard-going or frustrating at first, but it is very worthwhile. Skills in R or any other programming language are very valuable to all sorts of employers.\nAt the same time, there is one important thing to note:\nYour grade for any assessment will never be dependent on your coding ability.\nThis is not a coding course: some of you may pick it up straight away, and others may stay at a basic level throughout the course. That is completely fine. You are welcome to ask for help from me, your peers, and online. You can even ask a chatbot for coding help, if you like. In all your assessments, I’ll take into account your understanding of data science and visualisations on a more fundamental, humanities-led level.\nIt’s easy to get stuck or lost in assignments. When you’re learning a new skill like coding, you can spend many hours agonising over what might turn out to be a simple problem when it’s explained to you.\nThe key is to stick strictly to the number of hours given to each assignment - don’t waste lots of time working out what might turn out to be a small mistake."
  },
  {
    "objectID": "index.html#course-load",
    "href": "index.html#course-load",
    "title": "Information Visualization and the Humanities",
    "section": "Course Load",
    "text": "Course Load\nTotal course load 5 EC x 28 hours = 140 hours:\n\nSeminar: 13 x 2 (26 hours)\nCourse readings and coding practice (34 hours)\nAssignment(s): (35 hours)\nPeer Feedback: (5 hours)\nFinal project/paper: (40 hours)"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Information Visualization and the Humanities",
    "section": "Grading",
    "text": "Grading\n\nAssignments: 40 percent\nClass Participation & Peer Feedback: 20 percent\nFinal project: 40 percent"
  },
  {
    "objectID": "index.html#reading",
    "href": "index.html#reading",
    "title": "Information Visualization and the Humanities",
    "section": "Reading",
    "text": "Reading\nEach week will have one piece of assigned reading. The reading should be done before the week in question, e.g. Joanna Drucker should be read before our class in week 2. Some of these readings will be very practical, and others will be more abstract or conceptual.\nThe first 20 minutes of each class will involve a discussion on the reading. To facilitate this, I ask you to have prepared one question for each piece of reading, to show your engagement with the text.\n\nWeekly Reading Schedule\n\n\n\n\n\n\n\nWeek\nDate\nReading\n\n\n\n\n1\n2023-09-11\nIsabel Mereilles, Design for Information, ‘Introduction’ (ebook available through Library catalogue)\n\n\n2\n2023-09-18\nDay, Shawn. (2023) ‘Visualising humanities data’, in O’Sullivan, J. (ed.) The Bloomsbury Handbook to the Digital Humanities. London: Bloomsbury Publishing, pp. 211-219. (ebook available through the Library or open access version available here)\nJeffrey Heer, Michael Bostock, and Vadim Ogievetsky: A Tour Through the Visualization Zoo\n\n\n3\n2023-09-25\nEdward Tufte, The Visual Display of Quantitative Information (Cheshire: 2001): Chapter 1\nChapter 3, ‘On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints’, from the book Data Feminism (open access, available here)\n\n\n\n2023-10-09\nClaus Wilke, Fundamentals of Data Visualization (available online). Chapters 2, 3, and 4.\n\n\n\n2023-10-16\nOlivia Vane and Rosamund Pearce, ‘How to Visualise Sensitive Topics’\nAlex Selby-Boothroyd, ‘Mapping Refugee Flows’\nhttps://it.wisc.edu/learn/accessible-content-tech/accessible-data-visualizations/\n\n\n4\n2023-10-23\nField Trip - No Reading\n\n\n5\n2023-11-06\nIsabel Mereilles, Design for Information, ‘Chapter 4 (Spatial Structures: Maps)’ (ebook available through Library catalogue)\nJeppesen and Sartoretto, Cartographies of Resistance: Counter-Data Mapping as the New Frontier of Digital Media Activism\n\n\n6\n2023-11-13\nClaus Wilke, Fundamentals of Data Visualization (available online). Chapter 15.\n\n\n9\n2023-11-20\nIsabel Mereilles, Design for Information, ‘Chapter 2 (Relational Structures: Networks)’ (ebook available through Library catalogue)\n\n\n10\n2023-11-27\nVenturini, T., Jacomy, M., & Jensen, P. (2021). What do we see when we look at networks: Visual network analysis, relational ambiguity, and force-directed layouts. Big Data & Society, 8(1). https://doi.org/10.1177/20539517211018488\n\n\n11\n2023-12-04\nRead some style guides, e.g. the Economist, Urban Institute\nhttps://blog.datawrapper.de/text-in-data-visualizations/\n\n\n12\n2023-12-11"
  },
  {
    "objectID": "index.html#the-fine-print",
    "href": "index.html#the-fine-print",
    "title": "Information Visualization and the Humanities",
    "section": "The Fine Print",
    "text": "The Fine Print\nAttendance is required. If you know you will need to miss a class, please indicate this at least two weeks prior. If you know beforehand you will have to miss three or more classes, you cannot take this course. If you miss a class due to sickness or other unforeseen circumstances, please notify me without delay.\nClass participation is part of your final grade. Participation is evaluated both on attendance and on the quality of feedback given to the work of other students. Aside from participation being part of your grade, I really appreciate your input. If you think you have something to ask, please speak up: there are no stupid questions, but there are a lot of lost opportunities for information exchange and learning. This field is full of exciting new ideas and developments and it is impossible to be aware of them all. So, if you can share information on an idea, article, project, or tool that is of you value to you, please do!\nIf you need to speak to me, I’m happy to arrange a meeting to discuss any issues or problems you encounter on the course. Ideally, send a request a week in advance, via email. The meetings will probably take place in the meeting rooms in the top floor of the Arsenaal building.\nPlagiarism: If you have not done so already, please inform yourself on Leiden University’s views and regulations on plagiarism. This Leiden university library portal has several accessible web courses on how to quote and cite right and tips for bibliographic management. Note that plagiarism, copyright and other information sharing or copying issues are often extra complex when dealing with digital sources. If you are still in doubt whether (parts of) any work for this course may constitute plagiarism, you need to signal and verify this with me before you hand it in for grading.\n\nUse of Chatbots for assignments\nGPT-based chatbots such as ChatGPT or Google Bard are not banned outright but should be used with extreme caution. Due to the nature of this course, chatbot-generated text is unlikely to be of much use as you will be responding to your own visual work and that of your peers.\nChatbot-generated text will be treated as any other anonymous, secondary author. In terms of this course, its outputs are not considered reliable, verifiable information. You can quote or make reference to generated text as long as you cite it as a source (though I do not advise doing this because of the tendency to ‘hallucinations’). However you must not pass it off as your own original work: this will be treated as plagiarism. Extensive quoting, even if cited, will negatively impact your grade.\nOne exception to this is in coding: for any of your assignments, you are free to use and submit code from prompts given to ChatGPT or similar applications.\nIf you do use the output from a chatbot in any of your assignments, either for code help or other purposes, make a note of this in your submitted markdown documents, as a footnote."
  },
  {
    "objectID": "r-markdown.html#r-and-r-studio",
    "href": "r-markdown.html#r-and-r-studio",
    "title": "1  Introduction and Using R Markdown",
    "section": "R and R-Studio",
    "text": "R and R-Studio\nThroughout this course, we’ll mostly work on visualizations and data using the programming language R. This will be done using R-Studio, an interface designed to make R easier to work with (known as an IDE).\nFor this course, the data, files, and interface are all already set up for you in a workspace on a service called ‘Posit cloud’. Later on, you may want to install R and R-Studio on your local machine. See here for instructions on how to do this. The software is completely free and open-source.\n\nLogging into Posit Cloud and opening a notebook.\nThe first step is the create an account with Posit cloud. This is a service which will allow you to load R remotely, so you can complete this practical sessions for this course (you can also use R and R studio on your local machine if you use it already).\n\nSign up to Posit cloud with an email address here: https://posit.cloud/plans\nClick on ‘Learn more’ under the Free plan, and then ‘Sign up’.\nClick on the invite link. Once you have signed up, click on the ‘burger’ icon to the left of ‘Your Workspace’ to open a sidebar, which will allow you to switch to the correct workspace ‘Information Visualisation and the Humanities’:\n\nOnce you have switched, you’ll see a list of assignments and projects. At the moment, there is just one for this week. Click the start button for ‘Week 1: Introduction’.\n\nThis will load, and you should shortly be greeted with the Rstudio interface – this is an environment for creating and editing R code.\n\nR-Studio is divided into four different sections, or panes. Each of these also has multiple tabs. Starting from the top-left (numbered 1):\n\nThe source editor. Here is where you can edit R files such as RMarkdown or scripts.\nThe environment pane will display any objects you create or import here, along with basic information on their type and size.\nThis pane has a number of tabs. The default is files, which will show all the files in the current folder. You can use this to import or export additional files to R-Studio from your local machine.\nThe console allows you to type and execute R commands directly: do this by typing here and pressing return.\n\nAll four of these panes are important and worth it’s worth exploring more of the buttons and menu items. Throughout this course, you’ll complete exercises by using the source editor to edit notebooks. As you execute code in these notebooks, you’ll see objects pop into the environment pane. The console can be useful to test code that you don’t want to keep in a document. Lastly, getting to know how to use and navigate the directory structure using the files pane is essential.\nYou can play around with the R-Studio interface. Don’t worry about breaking anything, but you should regularly save your work."
  },
  {
    "objectID": "r-markdown.html#your-derived-workspace",
    "href": "r-markdown.html#your-derived-workspace",
    "title": "1  Introduction and Using R Markdown",
    "section": "Your Derived Workspace",
    "text": "Your Derived Workspace\nYou can return to the workspace overview by clicking on the sidebar and on the ‘Information Visualization[…]’ menu item again.\nIf you do this, you’ll see that there are now two workspace items. This is because when you started the workspace, it created a ‘derived’ version - meaning it has created a unique copy for you to work with, without changing the original one.\nYou can delete this version, copy it, or export a copy to your local machine. When you want to resume your work, you should open this copy. Please note that as the instructor, I can see and edit all the derived workspaces."
  },
  {
    "objectID": "r-markdown.html#r-markdown-and-knitting-a-document",
    "href": "r-markdown.html#r-markdown-and-knitting-a-document",
    "title": "1  Introduction and Using R Markdown",
    "section": "R Markdown and ‘Knitting’ a Document",
    "text": "R Markdown and ‘Knitting’ a Document\nThere are several ways to create and execute code within R studio, such as creating scripts or typing into the console (the bottom-left pane).\nHowever, in this course, we’ll exclusively use one method: R Markdown notebooks.\nThese are special documents which can include code and text. Text is written using a simple format called Markdown, and code goes into special ‘cells’ within the document. Once you’re finished, you’ll tell R studio to execute all the code in the document and render it as an html file. This process is called ‘knitting’.\nWe’ll worry more about code next week, but for now, let’s just get used to creating R Markdown files, knitting them, and downloading them to your local machine so they can be sent as assignments.\n\nCreate a new markdown file\nClick file -&gt; new file -&gt; R notebook.\n\nThis will open a markdown notebook in the top-left pane. If asked to install packages, just click yes, and wait a little longer.\nA notebook will now be open in your screen. This is where you can type text and code.\nAt the moment, there is some sample text and code in the document.\nAt the top of the document is some formatted text which looks like this:\n\nThis is called a yaml header. It contains information about your document, and it needs to remain here for the notebook to work. You can use this yaml header to edit the title of your report: just edit the text within the quotation marks which now reads ‘R Notebook’.\nYou should delete the rest of the text in the document: highlight and delete the rest of the contents of the notebook, using delete/backspace.\n\n\nType some text\nType some text. There are two ways of doing this. You can use the source editor, which is the default view. You can switch between Source and Visual using the button above.\n\nIn the source editor, use markdown. This is a very simple coding language used to render text on the internet. There are codes for bold, italics, headings. You can add images etc. You can switch between the two at any time.\n\nIn the visual editor, use the menu items to format your text. For example bold, italics, headings. You can use the drop-down menu to add images, tables, and so forth, as you would with word processor software such as Word or Google docs.\n\n\n\nSave the file\nIn order to render/knit the file, it needs to be saved first.\nClick file, save, and give it a name.\n\n\nNaming conventions.\nIn this course, you will be required to use good naming conventions. Save the file with an appropriate name which communicates the most important information about the file: who created it, and what it contains.\nOnce you have done this, you’ll see a new .Rmd file appear in the file browser, in the bottom-right pane:\n\n\n\n\nKnit the document\nNow, we’ll render the document. When a document is rendered, it restarts the R engine, runs through the code, and outputs the document. In this case, it will be very quick, as we haven’t written any code.\nClick the drop-down where it currently says ‘Preview’, and then ‘render HTML’. This will create a new file in your folder.\n\nThe knitted html file will preview in a new window. You can close this.\nMore importantly, there are now a total of three additional files in the file browser:\n\n\nThe file ending in .html is the knitted output of the markdown.\nThe file ending in .Rmd is the ‘source code’ of the final file.\nThe file ending in nb.html is created for making a quick preview, and can be ignored.\n\n\n\nDownload and save the file\nTo submit R Markdown files as assignments in Brightspace, you’ll need to download both the html output and the ‘raw’ markdown file to your local machine, and turn them both into a .zip file first.\nClick on the checkbox for both the html and the .Rmd file.\nClick on the ‘More’ dropdown menu, and then ‘Export’\n\nIf you have correctly selected more than one file, Posit cloud will automatically create a .zip file and download it. Save it with a sensible name, and then you can upload this zip directly to Brightspace.\nThis is the format we will use for every assignment, so it’s worth getting the hang of it!\n\n\nUpload to Brightspace\nUpload the .zip file to Brightspace in the assignment area."
  },
  {
    "objectID": "r-markdown.html#adding-code",
    "href": "r-markdown.html#adding-code",
    "title": "1  Introduction and Using R Markdown",
    "section": "Adding Code",
    "text": "Adding Code\nTo add a code block, use the menu items.\n\nThis will add a shaded area to your markdown file, called a ‘code cell’.\n\nAdd the simple code, 1 + 1. This will simply tell R to add these two numbers together.\nFirst, execute the code from the cell, without knitting the document. Click on the green triangle in the top-right of the code cell. You’ll see that the output (2) appears below the cell.\n\nThis is the code output. This output can be anything that we make in code, such as a visualisation. This is what makes R markdown such a great tool creating data visualisation reports. We can easily write up text and findings, and publish them alongside code results, tables, visualisations, even interactive maps.\nNext, render/knit the document as in the last step. The document will knit again. Look at the html file, and you’ll see that the code is displayed, and the code result - the output - is displayed underneath."
  },
  {
    "objectID": "r-markdown.html#adding-files",
    "href": "r-markdown.html#adding-files",
    "title": "1  Introduction and Using R Markdown",
    "section": "Adding files",
    "text": "Adding files\nThe last thing we’ll do is practise adding files to the Rstudio area. Most times, you’ll need to upload files to your workspace in order to work with them. First, you’ll up load them to the workspace, and second, you’ll read them into R.\nYou could also upload other files, for example if your final project markdown includes some images not generated by code.\nWe’ll practice this by downloading a file from Brightspace, and uploading it to Posit Cloud. This file can be found under the ‘Files’ area on the course site. Download the file ‘box_office’ to your local machine first.\nTo upload a file to Posit cloud, look at the file manager in the bottom-right pane. Click on ‘Upload’, and then browse to the file on your computer, and click OK. You’ll see the file appear in the file browser in a moment."
  },
  {
    "objectID": "r-markdown.html#first-weekly-task-identify-a-data-source",
    "href": "r-markdown.html#first-weekly-task-identify-a-data-source",
    "title": "1  Introduction and Using R Markdown",
    "section": "First weekly task: identify a data source",
    "text": "First weekly task: identify a data source\nTo practice creating and submitting R markdown documents, I would like you to write a very short report (50-100 words). You should spend thirty minutes to a maximum of one hour on this.\nFor this report, you’re asked to identify a dataset or data source you’d like to work with over the rest of the course. Ideally, this should be something you are interested in and have some existing knowledge of - this will make your life much easier later.\nSome examples are here:\n\nUN Data: The portal for all sorts of United Nations macro-scale data.\nUNHCR Refugee Data Finder. A recent favourite of mine, this resource contains data on displaced persons and flows from 1962 to 2022. It’s also available as a package for R, making it very easy to work with (I can help with this).\nRainbow Map: data on the legal status and social climate for LGBTI people in Europe (click download data button).\nOverheid.nl: Portal contain thousands of datasets published and shared by the Dutch government.\nIATI Registry: A portal where major developmental NGOs provide information on the projects they undertake in as transparent a way as possible.\nNatural Earth: Free raster and vector maps, including info on borders, names, and basic statistics. This is also available as a package in R.\nSEDAC: NASA’s Socio-Economic Data and Applications Center\nEuropean Data Portal: A portal that collects even more data from across the Public Sector in Europe.\nDANS: The KNAW’s (Koninklijke Nederlandse Akademie van Wetenschappen) data repository curating more than 200.000 data-sets based on (Dutch) academic research.\nThe Trans-Atlantic Slave Trade Database: A specific data-set focused on the records of 36.000 slaving voyages.\nConnected Histories: integrated search for British history sources from 1500-1900, including Old Bailey Online, the Digital Panopticon, and many more.\nClimatological Database of the World’s Oceans: dataset containing position and weather information from over 280,000 ship’s logbooks, dating from 1750 - 1850. Not only useful for climate research but an interesting geographical and historical dataset too!\nDelpher: Over 100 million digitized Dutch newspaper pages.\nStanford Large Network Dataset Collection: It’s both a large collection as well as a collection of large networks! \nKONECT: Another network collection (also large, not all of which are large networks) from the university of Koblenz.\nData.World: A commercial platform that hosts a large collection of data-sets on various sources. Not all data are open, however, and a sign-up is required.\n\nYou don’t need to stick to this data source if you realise it’s not suitable later.\nOnce you have identified your dataset (or general data source), open Posit cloud and create a new R markdown document. Write up a very short description of the data, including:\n\nThe name and url to the data - how do you access it?\nWhy you would like to work with it\nWhat stories you think you could tell with it\nAny potential problems that might occur, such as needing extensive cleaning, or issues with access.\n\nThe report should have a custom title and should contain at least 4 different markdown features, such as:\n\nMultiple section and sub-section headers.\nText in bold/italics\nAn image, such as a screenshot, or an example of the kind of visualisation you would like to create. To add images, you’ll first need to upload them to your workspace. You can do this using the ‘Upload’ button in the bottom-right pane (the file browser).\nSome text with a hyperlink.\nBullet points\nA table\n\nWhen you’re finished, download the .Rmd and the html, making sure it is a single .zip file. Rename this .zip file so that it begins with your surname, and upload it to Brightspace, under ‘Week 1: Create and submit an R Markdown document’.\nFinally, close the Posit Cloud workspace."
  },
  {
    "objectID": "r-intro.html#class-objectives",
    "href": "r-intro.html#class-objectives",
    "title": "2  Introduction to R",
    "section": "Class Objectives",
    "text": "Class Objectives\nThis week, we’re going to get used to the environment we’ll use for creating the visualisations for this course. By the end of today, you should be comfortable with the interface, and able to use R, load packages, and do some basic manipulations of dataframes.\n\nWhat if I get stuck?\nIt’s OK! Take your orange post-it, and place it on the back of your computer. I’ll come around to help as soon as possible.\nIf you’ve finished everything, put up your green post-it. I’ll give you some advanced exercises to complete."
  },
  {
    "objectID": "r-intro.html#load-r-studio-using-posit-cloud",
    "href": "r-intro.html#load-r-studio-using-posit-cloud",
    "title": "2  Introduction to R",
    "section": "Load R Studio using Posit cloud",
    "text": "Load R Studio using Posit cloud\nAs last week, we’ll open R using Posit cloud. Use the sidebar to switch to the ‘Information Visualisation and the Humanities’ Workspace, and click on the new assignment, for week 2, called ‘Week 2: Introduction to R’:\n\nYou’ll see an existing .Rmd file in the file manager in the bottom right. This is a ‘live’ version of this book chapter. It contains code cells and text. The first part of this practical session simply involves running these cells in the correct order, and looking at the output.\nYou should now switch to reading this in the .Rmd file in Posit studio. I’ll explain and demonstrate each cell, and then you can run them for yourself.\nFollowing that, you’ll create your own new markdown notebook, and practice some of the techniques we’ve learned- and submit it to Brightspace as a weekly task."
  },
  {
    "objectID": "r-intro.html#open-the-.rmd-file",
    "href": "r-intro.html#open-the-.rmd-file",
    "title": "2  Introduction to R",
    "section": "Open the .Rmd file",
    "text": "Open the .Rmd file\nTo begin, click on the .Rmd file. It will open in the reading pane in the top-left. The .Rmd file is an identical copy of this chapter, except you can see the ‘source’, and can change and run the code directly.\nIf asked to install packages, go ahead and install them. It might take a few moments to complete.\nYou can switch between two views - again as we saw last week. Either ‘Source’ or ‘Visual’. In the ‘Source’ view, you’ll see the source markdown code used to write the document. The ‘Visual’ view will show the page as it would look when finished, including the images.\nAs we learned in the previous week, .Rmd files contain ‘regular’ text, as well as code cells - these are shaded boxes with {r} in the left corner. On the right corner, they have a number of icons. Clicking the small green triangle icon will ‘run’ the cell, meaning any code in it will be executed and the output, if any, will be displayed underneath.\nLet’s try this here. Click on the green triangle to the right of the shaded code cell below this text:\n\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nThe code (which simply tells R to print the message “Hello World” should run, and the result should appear below."
  },
  {
    "objectID": "r-intro.html#r-code",
    "href": "r-intro.html#r-code",
    "title": "2  Introduction to R",
    "section": "R code",
    "text": "R code\nR code, as most programming languages, involves manipulating data. Data is usually stored as variables and the manipulations are usually done by applying these variables to functions. Often, programmers will write their own functions, but in this course, we’ll exclusively use existing functions, from various packages."
  },
  {
    "objectID": "r-intro.html#installing-and-loading-packages",
    "href": "r-intro.html#installing-and-loading-packages",
    "title": "2  Introduction to R",
    "section": "Installing and loading packages",
    "text": "Installing and loading packages\nBefore we begin, an important part of using R is knowing how to install and load packages. Packages are bundles of pre-made functions, usually with a specific task in mind, such as the sf package for geospatial analysis, or the dplyr package for data wrangling. Many times when using R you will use these packages.\nIn order to be able to use them, they need to first be installed, and second they need to be loaded.\nTo install a new package, use the code install.packages() with the package name inside, in quotation marks. For example to install the palmerpenguins package:\n\ninstall.packages('palmerpenguins')"
  },
  {
    "objectID": "r-intro.html#loading-existing-packages",
    "href": "r-intro.html#loading-existing-packages",
    "title": "2  Introduction to R",
    "section": "Loading existing packages",
    "text": "Loading existing packages\nOnce you have installed a package, you have to tell R that it should load the functions from the package into the ‘session’, so you can use them. Do this using the command library(). To load the tidyverse package, which we’ll use in this tutorial, run the following cell:\n\nlibrary(tidyverse)\n\nNote that now, the package name does not need to be in quotation marks.\n\n‘Base’ R.\nCommands using R without needing any additional packages are often called ‘base’ R. Here are some important ones to know:\nYou can assign a value to an object using = or &lt;-:\n\nx = 1\n\ny &lt;- 4\n\nEntering the name of a variable in the console and pressing return will return that value in the console. The same will happen if you enter it in a notebook cell (like here below), and run the cell. This is also true of any R object, such as a dataframe, vector, or list.\n\ny\n\n[1] 4\n\n\nYou can do basic calculations with +, -, * and /.\nThe code below creates a variable x, which is the result of the calculation 1 + 1, a variable y, which is the result of the calculation 4 - 2, and a variable z, which is the result of multiplying x by y. Finally, we print the output of z.\n\nx = 1 + 1\n\ny = 4 - 2\n\nz = x * y\n\nz\n\n[1] 4\n\n\n\nComparisons\nYou can compare numbers or variables using == (equals), &gt; (greater than), &lt;, (less than) != (not equal to). These return either TRUE or FALSE, which you’ll see if you run this cell:\n\n1 == 1\n\n[1] TRUE\n\nx &gt; y\n\n[1] FALSE\n\nx != z\n\n[1] TRUE\n\n\n\n\n\nBasic R data structures\nIt is worth understanding the main types of data that you’ll come across, in your environment window (top-right pane).\nA variable is a piece of data stored with a name, which can then be used for various purposes. It’s called a variable because we can change it and re-run the same code. The simplest of these are single elements, such as a number:\n\nx = 1\n\nx\n\n[1] 1\n\n\nNext is a vector. A vector is a list of elements. A vector is created with the command c(), with each item in the vector placed between the brackets, and followed by a comma. If your vector is a vector of words, the words need to be in inverted commas or quotation marks.\n\npiece = c(1,2,3,4,5,6,7,8,9,10,11,12)\nfruit = c(\"oranges\", \"apples\", \"oranges\", \"oranges\", \"apples\", \"oranges\", \"apples\", \"bananas\", \"bananas\", \"bananas\", \"oranges\", \"bananas\")\ncolour = c(\"orange\", \"red\", \"orange\", \"orange\", \"red\", \"orange\", \"green\", \"yellow\", \"yellow\", \"yellow\", \"orange\", \"yellow\")\ncost = c(5,10,4,6,9,5,11,3,3,4,2,2)\ncost_last_week = c(7,12,6,8,11,7,13,5,5,6,4,4)\n\nNext are dataframes. These are the spreadsheet-like objects, with rows and columns, which you’ll use in most analyses.\nYou can create a dataframe using the data.frame() command. You just need to pass the function each of your vectors, which will become your columns. Don’t worry about the code kableExtra::kbl() for now: it simply prints the table in a way which makes it very clear.\n\nfruit_data = data.frame(piece, fruit, colour, cost,cost_last_week, stringsAsFactors = FALSE)\n\nfruit_data %&gt;% kableExtra::kbl()\n\n\n\n\npiece\nfruit\ncolour\ncost\ncost_last_week\n\n\n\n\n1\noranges\norange\n5\n7\n\n\n2\napples\nred\n10\n12\n\n\n3\noranges\norange\n4\n6\n\n\n4\noranges\norange\n6\n8\n\n\n5\napples\nred\n9\n11\n\n\n6\noranges\norange\n5\n7\n\n\n7\napples\ngreen\n11\n13\n\n\n8\nbananas\nyellow\n3\n5\n\n\n9\nbananas\nyellow\n3\n5\n\n\n10\nbananas\nyellow\n4\n6\n\n\n11\noranges\norange\n2\n4\n\n\n12\nbananas\nyellow\n2\n4\n\n\n\n\n\n\n\nWe can also use the glimpse() or str() commands to view some basic information on the dataframe (particularly useful with longer data).\n\nglimpse(fruit_data)\n\nRows: 12\nColumns: 5\n$ piece          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ fruit          &lt;chr&gt; \"oranges\", \"apples\", \"oranges\", \"oranges\", \"apples\", \"o…\n$ colour         &lt;chr&gt; \"orange\", \"red\", \"orange\", \"orange\", \"red\", \"orange\", \"…\n$ cost           &lt;dbl&gt; 5, 10, 4, 6, 9, 5, 11, 3, 3, 4, 2, 2\n$ cost_last_week &lt;dbl&gt; 7, 12, 6, 8, 11, 7, 13, 5, 5, 6, 4, 4\n\n\n\n\nEnvironment\nEvery time we create an object like this, it will appear within RStudio’s environment. This means it has been saved in memory as an object which can be used for other purposes. When we close RStudio, the environment disappears.\nYou can see the objects in the environment in the top-right corner (the screenshot below will only show in the book version):\n\nHere you’ll see everything we’ve created, and a preview of what it contains. It’s divided into Data and Values. Values are the simple things we have created. We can see that x, for example, is a single number: 4. amount is a vector of numbers.\nData contains any dataframes we have created. We can see we have created one, called fruit_data. We can also click on the fruit_data object to open it:\n\nClicking on it will open a view of the dataframe in the top-left pane. You’ll see it looks much like a spreadsheet with rows and columns. This can be very useful to see how your data looks and how it has been imported.\nYou can close this tab or switch back to the .Rmd file to continue.\n\n\nData types\nReturn again at the output under the cell with the code glimpse(fruit_data). Notice that to the right of the third column, the amount, has &lt;dbl&gt;under it, whereas the other two have &lt;chr&gt;? That’s because R is treating the third as a number and others as a string of characters. It’s often important to know which data type your data is in: you can’t do arithmetic on characters, for example. R has 6 data types:\n\ncharacter\nnumeric (real or decimal)\ninteger\nlogical\ncomplex\nRaw\n\nThe most commonly-used ones we’ll use in this course are character, numeric, and logical. logical is data which is either TRUE or FALSE. In R, all the items in a vector are coerced to the same type. So if you try to make a vector with a combination of numbers and strings, the numbers will be converted to strings, as in the example below:\n\nfruit = c(\"apples\", 5, \"oranges\", 3)\n\nglimpse(fruit)\n\n chr [1:4] \"apples\" \"5\" \"oranges\" \"3\""
  },
  {
    "objectID": "r-intro.html#tidyverse",
    "href": "r-intro.html#tidyverse",
    "title": "2  Introduction to R",
    "section": "Tidyverse",
    "text": "Tidyverse\nMost of the work in this course will use a set of packages developed for R called the ‘tidyverse’. These enhance and improve a large range of R functions with a more intuitive syntax. The Tidyverse is really a ‘family’ of individual packages for sorting, filtering and plotting data frames.\nAll these functions work in the same way. The first argument is the thing you want to operate on. This is nearly always a data frame. After come other arguments, which are often specific columns, or certain variables you want to do something with.\n\nlibrary(tidyverse)\n\nHere are a couple of the most important ones\n\nselect(), pull()\nselect() allows you to select columns. You can use names or numbers to pick the columns, and you can use a - sign to select everything but a given column.\nUsing the fruit data frame we created above: We can select just the fruit and colour columns:\n\nselect(fruit_data, fruit, colour)\n\n     fruit colour\n1  oranges orange\n2   apples    red\n3  oranges orange\n4  oranges orange\n5   apples    red\n6  oranges orange\n7   apples  green\n8  bananas yellow\n9  bananas yellow\n10 bananas yellow\n11 oranges orange\n12 bananas yellow\n\n\nSelect everything but the colour column:\n\nselect(fruit_data, -colour)\n\n   piece   fruit cost cost_last_week\n1      1 oranges    5              7\n2      2  apples   10             12\n3      3 oranges    4              6\n4      4 oranges    6              8\n5      5  apples    9             11\n6      6 oranges    5              7\n7      7  apples   11             13\n8      8 bananas    3              5\n9      9 bananas    3              5\n10    10 bananas    4              6\n11    11 oranges    2              4\n12    12 bananas    2              4\n\n\nSelect the first two columns:\n\nselect(fruit_data, 1:2)\n\n   piece   fruit\n1      1 oranges\n2      2  apples\n3      3 oranges\n4      4 oranges\n5      5  apples\n6      6 oranges\n7      7  apples\n8      8 bananas\n9      9 bananas\n10    10 bananas\n11    11 oranges\n12    12 bananas\n\n\n\n\ngroup_by(), summarise(), tally()\nA very useful function is group_by(). By itself, it groups rows of your data together based on . Once this is done, you can apply a function to each group. A useful and simple example is tally(). Using tally() on your grouped dataset will count the number of rows in each group. The diagram below is an attempt to explain how grouping works. First, the group_by() function takes a dataset of fruit observations, and puts them into their relevant groups. Next, count() or tally() will count the number of fruit in each group.\n\nTo do this in R, first you need to create a new dataframe with the grouped fruit.\n\ngrouped_fruit = group_by(fruit_data, fruit)\n\nNext, use tally() on new dataset. This counts all the instances of each fruit group.\n\ntally(grouped_fruit)\n\n# A tibble: 3 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;int&gt;\n1 apples      3\n2 bananas     4\n3 oranges     5\n\n\nSee? Now the apples are grouped together rather than being two separate rows, and there’s a new column called n, which contains the result of the count.\nSometimes, we’ll want to add up the totals for some other measurement of the data. Imagine our fruit dataset also contained the cost of each fruit for each observation.\nWe could produce a new summary of the data which calculated the total cost for each group, with a small change to the method, as in this diagram:\n\nIf we specify that we want to count by something else, we can add that in as a ‘weight’, by adding wt = as an argument in the function.\n\ntally(grouped_fruit, wt = cost)\n\n# A tibble: 3 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apples     30\n2 bananas    12\n3 oranges    22\n\n\nThat counts the amounts of each fruit, ignoring the colour.\n\n\nfilter()\n\n\n\nSource: https://allisonhorst.com/r-packages-functions\n\n\nThe filter() function keeps only certain rows in a dataframe, based on a condition which you set within the function. This is very useful when working with data visualisations, because often you will want to focus on a small amount of the data, or perhaps a single category.\nIn the filter function, the first argument is the data to be filtered. The second is a condition (or multiple conditions). The function will return every row where that condition is true.\nJust red fruit:\n\nfilter(fruit_data, colour == 'red')\n\n  piece  fruit colour cost cost_last_week\n1     2 apples    red   10             12\n2     5 apples    red    9             11\n\n\nJust fruit which cost at least 5:\n\nfilter(fruit_data, cost &gt;=5)\n\n  piece   fruit colour cost cost_last_week\n1     1 oranges orange    5              7\n2     2  apples    red   10             12\n3     4 oranges orange    6              8\n4     5  apples    red    9             11\n5     6 oranges orange    5              7\n6     7  apples  green   11             13\n\n\nYou can also filter with multiple terms by using a vector (as above), and the special command %in%:\n\nfilter(fruit_data, colour %in% c('red', 'green'))\n\n  piece  fruit colour cost cost_last_week\n1     2 apples    red   10             12\n2     5 apples    red    9             11\n3     7 apples  green   11             13\n\n\n\n\nmutate()\nMutate creates new columns in your dataframe, based on existing columns or variables.\n\n\n\nSource: https://allisonhorst.com/r-packages-functions\n\n\nFor example, we can use mutate() to create a new column, adding the results of cost and cost_last_week. Here, we are telling mutate to create a new column called total_cost, and for each row, that column should be the result of adding together the other two columns:\n\nmutate(fruit_data, total_cost = cost + cost_last_week) %&gt;% \n  kableExtra::kbl()\n\n\n\n\npiece\nfruit\ncolour\ncost\ncost_last_week\ntotal_cost\n\n\n\n\n1\noranges\norange\n5\n7\n12\n\n\n2\napples\nred\n10\n12\n22\n\n\n3\noranges\norange\n4\n6\n10\n\n\n4\noranges\norange\n6\n8\n14\n\n\n5\napples\nred\n9\n11\n20\n\n\n6\noranges\norange\n5\n7\n12\n\n\n7\napples\ngreen\n11\n13\n24\n\n\n8\nbananas\nyellow\n3\n5\n8\n\n\n9\nbananas\nyellow\n3\n5\n8\n\n\n10\nbananas\nyellow\n4\n6\n10\n\n\n11\noranges\norange\n2\n4\n6\n\n\n12\nbananas\nyellow\n2\n4\n6\n\n\n\n\n\n\n\n\n\nslice_max(), slice_min()\nThese functions return the top or bottom number of rows, ordered by the data in a particular column. We set two parameters: order_by, which tells the function which value to use, and n, which specifies the number of rows we want to return.\n\nfruit_data %&gt;% slice_max(order_by = cost, n = 1)\n\n  piece  fruit colour cost cost_last_week\n1     7 apples  green   11             13\n\nfruit_data %&gt;% slice_min(order_by = cost, n = 1)\n\n  piece   fruit colour cost cost_last_week\n1    11 oranges orange    2              4\n2    12 bananas yellow    2              4\n\n\nThese can also be used with group_by(), to give the top rows for each group:\n\nfruit_data %&gt;% group_by(fruit) %&gt;% slice_max(order_by = cost, n  =  1)\n\n# A tibble: 3 × 5\n# Groups:   fruit [3]\n  piece fruit   colour  cost cost_last_week\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1     7 apples  green     11             13\n2    10 bananas yellow     4              6\n3     4 oranges orange     6              8\n\n\nNotice it has kept only one row per fruit type, meaning it has kept only the apple row with the highest amount.\n\n\nsort(), arrange()\nAnother useful set of functions, often you want to sort things. The function arrange() does this very nicely. You specify the data frame, and the variable you would like to sort by.\n\narrange(fruit_data, cost)\n\n   piece   fruit colour cost cost_last_week\n1     11 oranges orange    2              4\n2     12 bananas yellow    2              4\n3      8 bananas yellow    3              5\n4      9 bananas yellow    3              5\n5      3 oranges orange    4              6\n6     10 bananas yellow    4              6\n7      1 oranges orange    5              7\n8      6 oranges orange    5              7\n9      4 oranges orange    6              8\n10     5  apples    red    9             11\n11     2  apples    red   10             12\n12     7  apples  green   11             13\n\n\nSorting is ascending by default, but you can specify descending using desc():\n\narrange(fruit_data, desc(cost))\n\n   piece   fruit colour cost cost_last_week\n1      7  apples  green   11             13\n2      2  apples    red   10             12\n3      5  apples    red    9             11\n4      4 oranges orange    6              8\n5      1 oranges orange    5              7\n6      6 oranges orange    5              7\n7      3 oranges orange    4              6\n8     10 bananas yellow    4              6\n9      8 bananas yellow    3              5\n10     9 bananas yellow    3              5\n11    11 oranges orange    2              4\n12    12 bananas yellow    2              4\n\n\nIf you `sortarrange() by a list of characters, you’ll get alphabetical order:\n\narrange(fruit_data, fruit)\n\n   piece   fruit colour cost cost_last_week\n1      2  apples    red   10             12\n2      5  apples    red    9             11\n3      7  apples  green   11             13\n4      8 bananas yellow    3              5\n5      9 bananas yellow    3              5\n6     10 bananas yellow    4              6\n7     12 bananas yellow    2              4\n8      1 oranges orange    5              7\n9      3 oranges orange    4              6\n10     4 oranges orange    6              8\n11     6 oranges orange    5              7\n12    11 oranges orange    2              4\n\n\nYou can sort by multiple things:\n\narrange(fruit_data, fruit, desc(cost))\n\n   piece   fruit colour cost cost_last_week\n1      7  apples  green   11             13\n2      2  apples    red   10             12\n3      5  apples    red    9             11\n4     10 bananas yellow    4              6\n5      8 bananas yellow    3              5\n6      9 bananas yellow    3              5\n7     12 bananas yellow    2              4\n8      4 oranges orange    6              8\n9      1 oranges orange    5              7\n10     6 oranges orange    5              7\n11     3 oranges orange    4              6\n12    11 oranges orange    2              4\n\n\nNotice that now green apples are first.\n\n\nleft_join(), inner_join(), anti_join()\nAnother set of commands we’ll use quite often in this course are the join() ‘family’. Joins are a very powerful but simple way of selecting certain subsets of data, and adding information from multiple tables together.\nLet’s make a second table of information giving the delivery day for each fruit type:\n\nfruit_type = c('apples', 'bananas','oranges')\nweekday = c('Monday', 'Wednesday', 'Friday')\n\nfruit_days = data.frame(fruit_type, weekday, stringsAsFactors = FALSE)\n\nfruit_days\n\n  fruit_type   weekday\n1     apples    Monday\n2    bananas Wednesday\n3    oranges    Friday\n\n\nThis can be ‘joined’ to the fruit information, to add the new data on the delivery day, without having to edit the original table (or repeat the information for apples twice). This is done using left_join.\nJoins need a common key, a column which allows the join to match the data tables up. It’s important that these are unique (a person’s name makes a bad key by itself, for example, because it’s likely more than one person will share the same name). Usually, we use codes as the join keys. If the columns containing the join keys have different names (as ours do), specify them using the syntax below:\n\njoined_fruit = left_join(fruit_data, fruit_days, by = c(\"fruit\" = \"fruit_type\"))\n\njoined_fruit\n\n   piece   fruit colour cost cost_last_week   weekday\n1      1 oranges orange    5              7    Friday\n2      2  apples    red   10             12    Monday\n3      3 oranges orange    4              6    Friday\n4      4 oranges orange    6              8    Friday\n5      5  apples    red    9             11    Monday\n6      6 oranges orange    5              7    Friday\n7      7  apples  green   11             13    Monday\n8      8 bananas yellow    3              5 Wednesday\n9      9 bananas yellow    3              5 Wednesday\n10    10 bananas yellow    4              6 Wednesday\n11    11 oranges orange    2              4    Friday\n12    12 bananas yellow    2              4 Wednesday\n\n\nIn this new dataframe, the correct weekday is now listed beside the relevant fruit type.\n\n\nPiping\nAnother useful feature of the tidyverse is that you can ‘pipe’ commands through a bunch of functions, making it easier to follow the logical order of the code. This means that you can do one operation, and pass the result to another operation. The previous dataframe is passed as the first argument of the next function by using the pipe %&gt;% command. It works like this:\n\nfruit_data %&gt;% \n  filter(colour != 'yellow') %&gt;% # remove any yellow colour fruit\n  group_by(fruit) %&gt;% # group the fruit by type\n  tally(cost) %&gt;% # count each group\n  arrange(desc(n)) # arrange in descending order of the count\n\n# A tibble: 2 × 2\n  fruit       n\n  &lt;chr&gt;   &lt;dbl&gt;\n1 apples     30\n2 oranges    22\n\n\nThat code block, written in prose: “take fruit data, remove any yellow colour fruit, count the fruits by type and cost, and arrange in descending order of the total”"
  },
  {
    "objectID": "r-intro.html#reading-in-external-data",
    "href": "r-intro.html#reading-in-external-data",
    "title": "2  Introduction to R",
    "section": "Reading in external data",
    "text": "Reading in external data\nMost of the time, you’ll be working with external data sources. These most commonly come in the form of comma separated values (.csv) or tab separated values (.tsv). The tidyverse commands to read these are read_csv() and read_tsv. You can also use read_delim(), and specify the type of delimited using delim = ',' or delim = '/t. The path to the file is given as a string to the argument file=.\n\ndf = read_csv(file = 'top_movie.csv') # Read a .csv file as a network, specify the path to the file here.\n\ndf\n\nNotice that each column has a data type beside it, either  for text or  for numbers. This is important if you want to sort or run calculations on the data.\nAdvanced: str_¥"
  },
  {
    "objectID": "r-intro.html#exercises",
    "href": "r-intro.html#exercises",
    "title": "2  Introduction to R",
    "section": "Exercises",
    "text": "Exercises\nNow is a chance to practice R skills on your own.\nFirst, create a new markdown notebook, following the instructions from last week.\nNext, complete the following exercises. Type the name of the exercise, and then create a separate code cell for each one.\nYou don’t need to complete them all - get as far as you can in the rest of the time of the class, and submit. If you have unfinished code, you’ll need to delete it. .Rmd files only ‘knit’ when the code can successfully complete.\n\n\nLoad the tidyverse library into your markdown file (for any markdown file, you need to load all the packages you use within it. It doesn’t matter if you have already loaded the packages separately.\n\n\nExercise 1\nWhat will be the value of each variable after each statement in the following program? Write the answers as text directly in the Markdown document.\n\nmass &lt;- 47.5\nage &lt;- 122\nmass &lt;- mass * 2.3\nage &lt;- age - 20\n\n\n\nExercise 2\nEnter the code from the previous exercise into your .Rmd file, and write a command to compare mass to age (see the section on comparisons above). Is mass larger than age?\n\n\nCode\nmass &lt;- 47.5\nage &lt;- 122\nmass &lt;- mass * 2.3\nage &lt;- age - 20\n\nmass &gt; age\n\n\n\n\nExercise 3\nIn a new code cell, write code to install the gapminder library. When it is finished, load this library and the tidyverse library.\n\n\nCode\ninstall.packages('gapminder')\n\nlibrary(gapminder)\n\nlibrary(tidyverse)\n\n\n\n\nExercise 4\nUpload a file to your workspace. The file is called gapminder.csv and it is available here. Download it to your local machine first and then upload to RStudio.\n\n\nExercise 5\nIn a new code cell, read the file top_movie.csv into R. Give it the name df_movie.\n\n\nCode\ndf_movie = read_csv('top_movie.csv')\n\n\n\n\nExercise 6\nIn a new cell, create a new version of df_movie, called df_movie_cleaned which:\n\nOnly includes the years 2000 to 2020.\nOnly contains the year, movie, distributor and total_in_2022_dollars columns.\n\n\n\nCode\ndf_movie_cleaned = select(df_movie, year, movie, distributor, total_in_2022_dollars)\n\ndf_movie_cleaned = filter(df_movie_cleaned, year %in% 2000:2020)\n\n\n\n\nExercise 7\nSummarise the df_movie_cleaned data. Print the following under the cell output:\n\nA count of the number of titles in df_movie_cleaned per each distributor.\nA count of the total box office takings for each distributor.\n\n\n\nCode\ngrouped = group_by(df_movie_cleaned, distributor)\n\ntally(grouped)\n\n\n\n\nExercise 8\nSort the data. Sort by the total box office takings amount in descending order, and print this result under the cell.\n\n\nCode\narrange(tally(grouped), desc(n))\n\n\n\n\nExercise 9\nEdit df_movie_cleaned to only include the ten highest-ranked movies, by the total box office taking.\n\n\nCode\nslice_max(df_movie_cleaned, order_by = total_in_2022_dollars, n = 10)\n\n\n\n\nExercise 10\nYou’ll need to have installed and loaded the gapminder library from the above exercise to complete this. Gapminder is a dataset of basic statistics for world countries over the past half a century. To load it into R, make a new cell, and create a new object called gapminder_df using the code gapminder_df = gapminder. Check your enviroment to ensure it has loaded correctly.\nUsing the pipe function described in the section above, perform the following tasks in a single code cell:\n\nTake the gapminder dataset, filter to include only data from the continent Asia.\nSelect the country, continent, lifeExp and gdpPercap columns\nCreate a new column, called gdp_lifeExp, which is the result of dividing the gdpPercap column by the lifeExp column.\nCalculate the mean (i.e. average) life expectancy for each country.\n(if you’re finding this easy). filter() can be used to filter by much more complex conditions. One powerful tool is filtering by matching string patterns, using a special language called regular expressions. For a final challenge, take your new dataset of summarised life expectancies, and filter to include only countries which end in the letters stan. The function you need to include in your filter is str_detect().\n\n\n\nFinished?\nSave and knit the markdown file, submit under ‘weekly tasks’ for week 3."
  },
  {
    "objectID": "datavis-0.html#data-visualisation-principles",
    "href": "datavis-0.html#data-visualisation-principles",
    "title": "3  Data Visualisation 0",
    "section": "Data Visualisation Principles",
    "text": "Data Visualisation Principles\nThis week, we’ll begin working with data visualisations. We’ll introduce in a very general sense the different ways we can draw things with data visualisation software. We won’t think too much about the specifics of making charts for different purposes - just get used to the general syntax and most common chart types.\n\nAims of Information Design\nTo begin with, we should think about the purpose of data visualisation. Essentially, the purpose is to aid cognition, usually of large-scale or complex data which otherwise is difficult to grasp. Some examples of how it aids cognition include using infovis to:\n\nUnderstand, explore and remember information\nTake in large amounts of data at once\nMake comparisons\nRead millions of data points or understand complex structures (e.g. hierarchies or networks)\nUnderstand temporal information, e.g. logic of going from earlier to later time\n\nThe reason data visualisation works is because our brains tend to make judgements about values based on visual elements. For example, if given two circles, one small and one big, and are asked which we think represents the larger value, you would likely pick the bigger one.\nSimilarly, we associate ‘hotter’ colours with larger values than colder ones.\nWe associate transparency or opacity with strength. And so forth..\nWe can use these principles in data visualisations. By mapping data to these visual elements, we can aid in the understanding of a given dataset by viewers.\n\n\nMapping Data to Aesthetics\nAnother word for these visual elements are aesthetics. There are many possible aesthetics, but some of the most commonly-found in infovis are:\n\nAnother important one not pictured here is transparency.\nWe can use these aesthetics to tell lots of different stories within our data. For example, we can use them to indicate the magnitude of a value, to draw attention to different categories, or simply to highlight information we wish the viewer to pay attention to.\nSo how can we use each of these? Let’s look at some examples using a dataset containing observations about the physical characteristics of some penguins.\n\n\nPosition\nAll data visualisations need to be positioned in some kind of space. In this course, that space will be 2D, but 3D visualisations are also possible. In the example below, position is used to highlight a relationship between two variables. From a dataset of penguin observations, the bill length of the penguins is assigned to the x or horizontal position, and the flipper length is assigned to the y, or vertical position.\nThe resulting scatterplot shows clearly a relationship between the two variables: a larger bill generally means that the penguin also has larger flippers, and the other way around.\n\n\n\n\n\n\n\nSize\nSize is often mapped to amounts or magnitude (we naturally understand that these two things map together). In this case, we’ve taken the visualisation above, and mapped the body mass of each penguin to the size of the points. In this way, we can visualise more pieces of information about the penguins.\n\n\n\n\n\n\n\nColour\nColor can be used for many purposes in visualisations. Here, it is used to distinguish between two categories: male and female penguins.\n\n\n\n\n\n\n\nLinetype/width\n\n\n\n\n\nHere, linetype is used to distinguish between the sex attribute.\n\n\nShape\nShape is usually used to distinguish between categories. Here, the sex of the penguin is mapped to different shapes. This might be useful if colour was not an option, for example:\n\n\n\n\n\n\n\nTransparency\nFinally, transparency. Here, the opacity of the points are assigned to the body mass of the penguins."
  },
  {
    "objectID": "datavis-0.html#scales",
    "href": "datavis-0.html#scales",
    "title": "3  Data Visualisation 0",
    "section": "Scales",
    "text": "Scales\nIn order to correctly map data to aesthetics, we use what are called scales. A scale defines a unique mapping between data and aesthetics. A scale might say, for example, each year of a line chart should correspond to exactly 50 pixels in a chart. One important distinction between data visualisation and say, graphic design, is that scales - what is mapped to data and how - need to be consistent within a chart.\nHow exactly the scales are picked depends on the type of data. Generally, there are two types of scales: continuous and discrete. Most of the time, these values are picked by ggplot2 software for you, but understanding how they work is important.\nContinuous scales are ones where arbitrarily small intermediates exist, for example time and weight. They are generally numerical, or quantitative data.\nDiscrete scales are ones where only finite numbers exist. Often, but not always, these are categorical, or qualititative data. For example, nationality, or the genre of a book.\nHowever, discrete scales can be numerical too: for example, a count of your siblings (you can’t have 2.4 siblings).\nSome aesthetics are generally only mapped to one or the other.\nSize, for example, should usually be continuous - it needs a numerical value set to the size of something.\nShape is the opposite. It doesn’t make sense to map continuous variables to shapes - there are only a limited, set number of shapes.\nPosition can be either, which we’ll go through in examples later.\n\nColour Scales\nColour is perhaps the most complicated and interesting. Colour is frequently mapped to both continuous and discrete scales, but behaves differently depending on which one is used. For instance, a continuous colour scale would mean one where any possible value is possible. This is usually where something like hue is mapped to a numerical value. A discrete colour scale is one where colour is used to distinguish between categories.\nTo illustrate the difference between the two colour scales, take these two data maps of the US.\nThe first maps a discrete or qualitative piece of data: whether a county voted a majority for the Republican candidate in the 2020 election, or the Democrat.\nThe data being mapped is a simple text category, either ‘Republican’ or ‘Democrat’.\nThe correct colour scale for this is discrete. The colours are not mapped to a numerical value, but chosen to be easily distinguishable from each other, for each category. Red and green, used here, are on opposite sides of the colour wheel and are easy to distinguish (except for some of those with colour vision deficiency, but we’ll deal with that later).\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nus_counties = st_read('ne_10m_admin_2_counties/ne_10m_admin_2_counties.shp', quiet = TRUE)\n\nelection_2020 = read_csv('US_County_Level_Election_Results_08-20-master/2020_US_County_Level_Presidential_Results.csv')\nelection_2020 = election_2020 %&gt;% distinct(county_name, .keep_all = TRUE) %&gt;% \n  filter(!state_name %in% c('Alaska', 'Hawaii'))\n\nus_map = us_counties %&gt;% \n  inner_join(election_2020, by = c('NAME_ALT' = 'county_name')) \n\nus_map = us_map %&gt;%\n  mutate(result = ifelse(votes_gop &gt; votes_dem, 'Republican', 'Democrat'))\n\nus_map %&gt;% ggplot() + geom_sf(aes(fill = result)) + \n  labs(fill = 'Result: ', title = \"US Presidential Election Results by County, 2020\") + theme_void()\n\n\n\n\n\nIn this second map, we visualise the mean household income for the same set of US counties. This data value is quantitative and numerical.\nThe correct colour scale for this map is a continuous one. The range of values are mapped to a spectrum of colours running from dark blue to light yellow: the higher the income, the lighter the hue on this spectrum. In this way, we can easily distinguish the highest-income, the lower-income, and those in between.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nus_counties = st_read('ne_10m_admin_2_counties/ne_10m_admin_2_counties.shp', quiet = TRUE)\n\ncounty_data = usdata::county_2019\n\ncounty_data = county_data %&gt;% \n  filter(!state %in% c('Alaska', 'Hawaii'))\n\nus_map = us_counties %&gt;% \n  inner_join(county_data, by = c('NAME_ALT' = 'name')) \n\nus_map %&gt;% \n  ggplot() + \n  geom_sf(aes(fill = mean_household_income, color =mean_household_income)) + \n  theme_void() + \n  labs(fill = '', title = \"Mean Household Income per County, 2020 (in dollars)\") + \n  scale_color_viridis_c(guide = 'none') + \n  scale_fill_viridis_c()\n\n\n\n\n\nWe’ll return to colour scales in more detail, later in the course."
  },
  {
    "objectID": "datavis-0.html#putting-this-into-practice",
    "href": "datavis-0.html#putting-this-into-practice",
    "title": "3  Data Visualisation 0",
    "section": "Putting this into practice",
    "text": "Putting this into practice\nSimilarly to last week, go to your Posit cloud workspace, and open the assignment called Week 3: data visualisation 0. As before, there is an existing .Rmd file, which contains the same code and text as this book chapter. You can use this to follow along and run the code yourself. For the exercises, create a new .Rmd file. Create new code cells and enter your answers within them. When you’re finished, knit and export the .Rmd and html output.\n\nLoad ggplot2\nAll the visualisations we create on this course will be made using a package for R called ggplot2. Before we do anything else, you’ll need to load this package. The easiest way to do this is load the ‘meta-package’ tidyverse, which will include ggplot2.\nAs last week, use library() to load the package:\n\nlibrary(tidyverse)\n\nThe basis of all ggplot2 visualisations is a function called ggplot(). Think of this as the foundation of your visualisation. You’ll tell the ggplot() function what data it should use and which attributes of that data it should map to specific aesthetics.\nThe ggplot() function doesn’t produce any visualisation on its own, however. For that, you’ll need to add what are called ‘geoms’. A geom can be thought of as the further ‘layers’ you build on top of the ggplot() foundation.\nIn simple terms, geometries can be considered as shape types: some examples include columns, points, or lines. When we connect these to our data correctly, we’ll produce data visualisations.\n\n\nBar charts with geom_col\nAs an example, let’s draw a simple bar chart using ggplot. We want to make a simple bar chart from the top box office movies data we used last week: each movie will be a separate bar, organised horizontally, and the height of each bar should correspond to the total box office takings for that movie.\nIn ggplot, a bar chart is created using the geom geom_col(). geom_col(), like all geoms, has a set of aesthetics which must be specified. In this case, we must at least specific the x and y aesthetics. These correspond to the horizontal and vertical position of the bars. When we give these aesthetics to geom_col, it will automatically figure out what to do with them.\nFirst, load the box office data as a dataframe:\n\ntop_movies = read_csv('top_movie.csv')\n\nRows: 29 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): movie, distributor\ndbl (4): year, total_for_year, total_in_2022_dollars, tickets_sold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntop_movies %&gt;% kableExtra::kbl()\n\n\n\n\nyear\nmovie\ndistributor\ntotal_for_year\ntotal_in_2022_dollars\ntickets_sold\n\n\n\n\n1995\nBatman Forever\nWarner Bros.\n184.0311\n445.4822\n42306002\n\n\n1996\nIndependence Day\n20th Century Fox\n306.1693\n729.4032\n69269062\n\n\n1997\nMen in Black\nSony Pictures\n250.6501\n575.0207\n54607854\n\n\n1998\nTitanic\nParamount Pictures\n443.3191\n995.3411\n94524324\n\n\n1999\nStar Wars Ep. I: The Phantom Menace\n20th Century Fox\n430.4434\n892.2379\n84732942\n\n\n2000\nHow the Grinch Stole Christmas\nUniversal\n253.3675\n494.9832\n47006948\n\n\n2001\nHarry Potter and the Sorcerer’s Stone\nWarner Bros.\n300.4044\n558.8796\n53074988\n\n\n2002\nSpider-Man\nSony Pictures\n403.7064\n731.6744\n69484746\n\n\n2003\nFinding Nemo\nWalt Disney\n339.7144\n593.2325\n56337374\n\n\n2004\nShrek 2\nDreamworks SKG\n441.2262\n748.1662\n71050925\n\n\n2005\nStar Wars Ep. III: Revenge of the Sith\n20th Century Fox\n380.2706\n624.6878\n59324582\n\n\n2006\nPirates of the Caribbean: Dead Man’s Chest\nWalt Disney\n423.3158\n680.5367\n64628368\n\n\n2007\nSpider-Man 3\nSony Pictures\n336.5303\n515.0675\n48914288\n\n\n2008\nThe Dark Knight\nWarner Bros.\n531.0016\n778.7530\n73955652\n\n\n2009\nTransformers: Revenge of the Fallen\nParamount Pictures\n402.1119\n564.5651\n53614916\n\n\n2010\nToy Story 3\nWalt Disney\n415.0049\n553.8658\n52598844\n\n\n2011\nHarry Potter and the Deathly Hallows: Part II\nWarner Bros.\n381.0112\n505.9329\n48046812\n\n\n2012\nThe Avengers\nWalt Disney\n623.3579\n824.6179\n78311295\n\n\n2013\nIron Man 3\nWalt Disney\n408.9923\n529.7280\n50306552\n\n\n2014\nGuardians of the Galaxy\nWalt Disney\n333.0553\n429.2622\n40765637\n\n\n2015\nStar Wars Ep. VII: The Force Awakens\nWalt Disney\n742.2089\n927.1008\n88043765\n\n\n2016\nFinding Dory\nWalt Disney\n486.2956\n591.9875\n56219140\n\n\n2017\nStar Wars Ep. VIII: The Last Jedi\nWalt Disney\n517.2184\n607.1694\n57660910\n\n\n2018\nBlack Panther\nWalt Disney\n700.0596\n809.1797\n76845177\n\n\n2019\nAvengers: Endgame\nWalt Disney\n858.3730\n986.7541\n93708843\n\n\n2020\nBad Boys For Life\nSony Pictures\n204.4179\n228.7481\n21723470\n\n\n2021\nSpider-Man: No Way Home\nSony Pictures\n572.9848\n580.1471\n55094689\n\n\n2022\nTop Gun: Maverick\nParamount Pictures\n718.7328\n724.2351\n68778260\n\n\n2023\nBarbie\nWarner Bros.\n594.8010\n594.8010\n56486324\n\n\n\n\n\n\n\nIn order to visualise this, we’ll tell ggplot to use the movie column as the x position, and the total_in_2022_dollars as the y position.\nThe first step is to tell the ggplot() function which dataframe it should use. To do this, set the data parameter to the top_movies object, as in the code cell below.\nNext we tell ggplot which columns it should use, and which aesthetics it should map these columsn to. This is also done by passing the names as parameters to x and y. First, type a comma after the data = top_movies code. Then, insert the code aes(). This tells ggplot it should interpret anything within these parentheses as aesthetic mappings. Finally, within the aes(), we’ll add the columns and aesthetics: x = movie, y = total_in_2022_dollars:\n\nggplot(data = top_movies, aes(x = movie, y = total_in_2022_dollars)) \n\n\n\n\nAt this point, we have an empty visualisation which specifies the aesthetics which should be mapped. In order to draw the correct geom, or visualisation shape, we need to add it as a layer on top of the ggplot() function. To do this, we add a plus sign (+) followed by the code geom_col():\n\nggplot(data = top_movies, aes(x = movie, y = total_in_2022_dollars)) + \n  geom_col()\n\n\n\n\nNow, we have a simple bar chart. You’ll notice that the defaults ggplot chooses are not always great ones: the names are displayed horizontally and are drawn over each other. The movies are also ordered alphabetically. We won’t worry about this now, but these things are very easy to change later.\n\n\nScatterplots with geom_point\nNext, let’s try another geom and see the difference. This time, we’ll draw a scatterplot. Scatterplots are often used to draw attention to some kind of relationship between two variables. In ggplot, scatterplots are drawn using geom_point(). As with geom_col, we need to at a minimum set the x and y aesthetics. In geom_point(), these refer to the x and y coordinates of the points. For our scatterplot, we want to plot the ticket sales against the takings, to see the relationship between the two.\nAs before, we start with ggplot(), and tell it what to use as the x and y:\n\nggplot(data = top_movies, aes(x = tickets_sold, y = total_in_2022_dollars))  + \n  geom_point()\n\n\n\n\nUnsurprisingly, there’s an almost perfectly straight line: the more tickets sold, the higher the takings…\n\n\nLine charts with geom_line\nAs a final example, let’s use geom_line(). This draws a line between points in some kind of order. It’s often useful to show some kind of trend, such as change over time.\nWe’ll draw a chart which draws a line through all the year values:\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars)) + \n  geom_line()\n\n\n\n\nIn this example, the line chart clearly shows the huge drop in the first pandemic year, 2020.\n\n\nColor\nWe can add new aesthetics to our visualisations as well as the positions (x and y). The most common additional one is colour, which can be used to represent a value in our data.\n\n\nWhat is aes()?\nThis is a good point to explain why we put x and y within aes(). By doing so, we tell ggplot that it should look at the data, and relate the aesthetic to the data itself.\nHowever in some cases, we just want to set all of the chart to a certain fixed value - the colour or transparency, for example. In this case, we put the aesthetics within the geom itself, and not within aes(), When we do this, we specify a particular value, rather than some value in the dataset.\nAs an example, here is how we would set all the columns in our first bar chart to blue. The aesthetic we use is fill, which sets the fill colour of the bars (to set the colour of the border, we would use the color aesthetic).\n\nggplot(data = top_movies, aes(x = movie, y = total_in_2022_dollars)) + \n  geom_col(fill = 'blue')\n\n\n\n\nHowever, in many cases we’ll want to map the colour to a value in the data. To do this, we will set the fill aesthetic within the aes(), as we did with x and y. To set the fill to the distributor column:\n\nggplot(data = top_movies, aes(x = movie, y = total_in_2022_dollars,fill = distributor)) + \n  geom_col()\n\n\n\n\nNow the column is coloured according to the distributor, and a legend has been automatically added.\n\n\nScales\nThe last important general point about ggplot is how it deals with different types of data. As mentioned above, data is generally found in either categorical or continuous forms. Categorical data that we have includes the distributor (a movie only has one distributor or another; it can’t be somewhere in between). Continuous data includes the box office totals (in principle, any value is possible). The year could potentially be interpreted as either.\nGgplot will automatically make some changes to some aspects of your plots depending on the data type.\nIn the bar chart above, the columns were coloured by a cattegorical variable: distributor. This means that ggplot gives a distinctive colour to each distibutor, so we can easily tell them apart. Let’s try, for the sake of it, to give it a numerical value. Ggplot will automatically interpret this as continuous:\n\nggplot(data = top_movies, aes(x = movie, y = total_in_2022_dollars,fill = year)) + \n  geom_col()\n\n\n\n\nThe difference is that now ggplot has chosen a sequential colour scheme: the colours run in a continuous hue from dark to light blue, depending on the numerical value of the year. Earlier years (lower values) have darker colours and higher values have lighter colours.\nThe interpretation of continuous and categorical data effects the scales too. Scales are the labels on the x and y axes.\nA numerical (continuous) variable will by default be ordered in order of the values from low to high. Rather than write out every number, ggplot will display numbers as what it has chosen as a sensible interval. This is because we can infer other values in between.\nA categorical (text) variable will be, by default, ordered alphabetically. Each separate value will be written out, because it doesn’t make sense to only include intervals, as we can with continuous data.\n\n\nExercises:\nTime to practice. These exercises will get progressively more difficult, and for some you’ll need to refer to the introduction to R last week, as they involve basic data wrangling. Give yourself an hour, and just try and complete as many as you can - there’s no need to complete everything.\nCreate a new .Rmd file. Complete each exercise in code blocks within this. If you get stuck, the code answers for each are included. But please try to complete each without help first of all!\n\nExercise 1\nLoad the tidyverse package\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nExercise 2\nRead the gapminder.csv data into R. You should have this file already on your local machine. To download it, click here. Upload it to your work space.\nSave it as an object called gapminder_df\n\n\nCode\ngapminder_df = read_csv('gapminder_data.csv')\n\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, pop, lifeExp, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nExercise 3\nOpen gapminder_df in your environment. Write the number of columns it contains in plain text (not within a code block).\n\n\nExercise 4\nRestrict the data to latest year in the data: 2007. Call it a new object gapminder_2007_df.\nIn later exercises you’ll need to do this yourself, but for this first one I have given you the code. Copy the below code into a new cell in your .Rmd file and run it:\n\n\nCode\ngapminder_2007_df = gapminder_df %&gt;% filter(year == 2007)\n\n\n\n\nExercise 5\nUsing this newly-created dataframe, draw a bar chart which plots the country column horizontally and population as the bar height.\n\n\nCode\nggplot(data = gapminder_2007_df, aes(x = country, y = pop)) + \n  geom_col()\n\n\n\n\nExercise 6\nIn a new code cell, create a scatterplot. Assign the x position to the lifeExp column, and the y position to the gdpPercap column.\n\n\nCode\nggplot(data = gapminder_2007_df, aes(x = lifeExp, y = gdpPercap)) + \n  geom_point()\n\n\nHow would you interpret this data visualisation? Write the answer below your code.\n\n\nExercise 7\nMake some adjustments to the scatterplot. Copy your existing scatterplot code to a new cell. This time, assign the pop to the size of the points, and the continent to the colour.\n\n\nCode\nggplot(data = gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = continent, size = pop)) + \n  geom_point()\n\n\nIF YOU GET TO THIS STAGE, WELL DONE!\nThe last few might take a bit longer to work out if you are a complete beginner. I also haven’t included the code solutions. You are welcome to try, or just stop here.\n\n\nExercise 8\nReturn to the original gapminder_df object.\nCreate a new object, gapminder_netherlands_df. Filter the full data to only include rows where the country is Netherlands.\n\n\nExercise 9\nDraw a plot which shows the change in the lifeExp variable in the Netherlands over time.\nYou should choose an appropriate geom for your plot.\n\n\nExercise 10\nLook at the dataframe in your environment, and choose a set of five countries. Filter the data (create a new dataframe object if needed) to only these five countries.\nDraw a chart which compares the life expectancy of those countries and its change over time.\n\n\nExercise 11 (bonus, will require external information!)\nUse summarise() to calculate the average life expectancy for each continent, for each year. Save this as a new object called average_life_exp.\nThis chart should contain four separate panels: one for each of the continents of Asia, Europe, Americas, and Africa (e.g., you need to remove Oceania). For each panel, use a line chart to separately display the change in life expectancy over time, for each country. Use the original gapminder data for this. Render the data for each country as a thin, black line.\nNext, use the dataframe with the averages you created above to draw an extra layer, of the average life expectancy. This should be drawn as a thicker, blue line, with an opacity (alpha) of 50%.\nTo do this, you’ll probably need the following information:\n\nTo render each country’s data as a separate line, you’ll have to specify the group = country within the aes().\nTo render each continent as a separate panel, you’ll need the layer facet_wrap(). You can find out more information about how to use this here: https://ggplot2-book.org/getting-started#sec-qplot-faceting\nTo add both the individual and average values to the plot, you’ll need to add them both as separate layers. This is done using the + symbol. However, because each layer is using a separate data source, rather than include the information on aesthetics and so forth within the ggplot() function, you’ll need to move this information, in the same format, to each of the individual geom_ layers.\n\n\n\n\n\n\nIf you’d like another challenge:\n\nRe-do the above, but don’t create a new dataframe with the averages. Include the averages as an additional column in the original dataframe. You’ll need to use group_by() and mutate() for this. You’ll also need to adjust the ggplot code.\n\n\n\nFinished?\nRender your .Rmd file, and submit the .Rmd and the html output under the weekly task.\nIf you struggle with ‘knitting’ the .Rmd file because of errors, just submit the .Rmd file on its own."
  },
  {
    "objectID": "data-visualisation-1.html#position-scales",
    "href": "data-visualisation-1.html#position-scales",
    "title": "4  Data Visualisation 1",
    "section": "Position scales",
    "text": "Position scales\nThe most common scales in data visualisation are numerical position scales.\nThe default scales are called scale_x_continuous and scale_y_continuous, which simply map linearly from a data value to a location on the plot.\nWe generally don’t need to think about these much. For example, let’s load up the movies database used last week and make a basic plot.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntop_movies = read_csv('top_movie.csv')\n\nRows: 29 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): movie, distributor\ndbl (4): year, total_for_year, total_in_2022_dollars, tickets_sold\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars)) + \n  geom_line()\n\n\n\n\nBehind the scenes, ggplot tells the function to map the total in 2022 dollars to the y axis, and the year to the x axis, both linearly (1 year/dollar always represents the same amount of space).\nThere are two useful things we can change with the default continuous scale: limits and breaks.\n\nLimits specify the limits of the scale - the start and end points of the x and y axes.\nBreaks specify which points and ticks should be drawn on the x and y axes.\n\nTo make changes to the default scale, we add scale_x_continuous or scale_y_continuous following a + sign to the plot.\nTo change the limits, add limits = followed by the start and end point of the limit you want, within c(), as in the example below:\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars)) + \n  geom_line() + \n  scale_x_continuous(limits = c(2000, 2010))\n\nWarning: Removed 18 rows containing missing values (`geom_line()`).\n\n\n\n\n\nYou can see that it has limited the years to 2000 at earliest and 2010 at the latest.\nYou’ll also notice that it has changed the labels and tick marks on the x axis. The new ones are not very elegant for a dataset of years - we don’t have any data for 2007.5 for example, so it looks misleading.\nThis leads on to the second element of the scale we can change: the breaks.\nChange the breaks by adding breaks = to the plot, followed by a vector (a list) of the values you want to specify. Note it doesn’t change anything about the chart except aesthetically.\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars)) + \n  geom_line() + \n  scale_x_continuous(limits = c(2000, 2010), breaks = c(2000, 2005, 2010))\n\nWarning: Removed 18 rows containing missing values (`geom_line()`).\n\n\n\n\n\nYou could also use a slightly different method to show all the full years:\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars)) + \n  geom_line() + \n  scale_x_continuous(limits = c(2000, 2010), breaks = 2000:2010)\n\nWarning: Removed 18 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAlso note that it changes the vertical lines in the plot. By default, ggplot draws a line for each break, as well as a fainter line in between each break.\nReordering data\nAnother useful adjustment you can make to plots is reordering.\nBy default, ggplot orders text data alphabetically. Remember another plot from last week:\n\nggplot(data = top_movies, aes(x = movie,y = total_in_2022_dollars)) + \n  geom_col()\n\n\n\n\nIn many cases, it is more useful to order the bars by numerical data in the plot. We can do this using reorder() in the following way:\n\nggplot(data = top_movies, aes(x = reorder(movie, total_in_2022_dollars), y = total_in_2022_dollars)) + \n  geom_col()"
  },
  {
    "objectID": "data-visualisation-1.html#coord_flip",
    "href": "data-visualisation-1.html#coord_flip",
    "title": "4  Data Visualisation 1",
    "section": "Coord_flip()",
    "text": "Coord_flip()\nA final useful function is coord_flip. This flips the coordinates of a plot, and reverses the position of the x and y axes. This is how it works:\n\nggplot(data = top_movies, aes(x = reorder(movie, total_in_2022_dollars),y = total_in_2022_dollars)) + \n  geom_col() + coord_flip()\n\n\n\n\nThis is particularly useful for a chart like this, because the labels are much easier to read when they are placed along the y-axis.\nNote that this doesn’t actually change the x and y axis under the hood, but just flips how it is drawn (meaning if you want to make changes to scales, you would change the horizontal using the y scale, and vice versa)."
  },
  {
    "objectID": "data-visualisation-1.html#color-scales",
    "href": "data-visualisation-1.html#color-scales",
    "title": "4  Data Visualisation 1",
    "section": "Color scales",
    "text": "Color scales\nThe other key scale we will work with are colour scales. As with position scales, ggplot will pick some default values, but we can change them.\nGGplot will pick different basic scales depending on how the data looks. For instance, if we map colour to text data (like a category), it will give a discrete scale - each colour is meant to be easily distinguished from the last. We’ll load the ‘gapminder’ dataset to illustrate this:\n\ngapminder_df = read_csv('gapminder_data.csv')\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, pop, lifeExp, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ngapminder_2007_df = gapminder_df %&gt;% filter(year == 2007)\n\nFirst, we’ll tell ggplot to map the color variable to the population (pop) column:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point()\n\n\n\n\nNext, we’ll tell it to map to the continent column:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = continent), size = 4)  + \n  geom_point()\n\n\n\n\nIn both cases, a different scale has been used. Ggplot also gives a legend, which guides the reader to how the data has been mapped to colours. Both the scale and legend can be changed.\nThis is done slightly differently depending on whether it’s a continuous or discrete scale:\nChange to a specific palette:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_viridis_c()\n\n\n\n\nUse scale_color_distiller to specify that ggplot should take a particular palette and distribute the values evenly along it. The full list of palettes can be found here: https://r-graph-gallery.com/38-rcolorbrewers-palettes.html\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_distiller(palette = \"RdPu\")\n\n\n\n\nYou can also ‘make your own’ palette. For instance, scale_color_gradient will create a gradient colour scale in between two colours you specify. A large list of R colour names can be found here: https://r-charts.com/colors/\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_gradient(low = 'firebrick', high ='gold')\n\n\n\n\nAn alternative is to use scale_gradient2(), which allows you to specify 3 colours, a low, mid and high. The function will create a scale similar to above but with a specified midpoint:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_gradient2(low = 'firebrick', mid = 'white', high ='gold')\n\n\n\n\nAs with position scales, we can set limits and breaks. In this case, we’ll see the change in the legend primarily.\nTo set the limit, specify within the scale_x_ or scale_y function you are using. In this, any values outside the limits will be coloured by a default NA value, in this case grey.\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_continuous(limits = c(500000,5000000 ))\n\n\n\n\nSet the breaks using the below:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_continuous(breaks = c(100000000, 200000000, 300000000, 400000000, 500000000, 1000000000))\n\n\n\n\nYou can make changes to the legend in other ways, using the guides() function. Here are some examples:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_continuous(breaks = c(100000000, 200000000, 300000000, 400000000, 500000000, 1000000000))+\n  guides(colour = guide_colourbar(reverse = TRUE))\n\n\n\n\nUse barheight to set the height of the legend (you can use barwidth to set the width):\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_continuous(breaks = c(100000000, 200000000, 300000000, 400000000, 500000000, 1000000000)) +\n  guides(colour = guide_colourbar(barheight = unit(5, \"cm\")))\n\n\n\n\nChange from vertical to horizontal:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = pop), size = 4)  + \n  geom_point() + \n  scale_color_continuous(breaks = c(100000000, 200000000, 300000000, 400000000, 500000000, 1000000000)) +\n  guides(colour = guide_colourbar(direction = \"horizontal\"))"
  },
  {
    "objectID": "data-visualisation-1.html#discrete-scales",
    "href": "data-visualisation-1.html#discrete-scales",
    "title": "4  Data Visualisation 1",
    "section": "Discrete scales",
    "text": "Discrete scales\nIn these cases, the color scale is continuous - the colour is mapped in a continuous gradient using hue, chroma, and luminance.\nWe can also make changes to discrete scales - where text or categories are used. By default, when ggplot uses a discrete scale, it will try to evenly space out the values so they are equally spaced around the colour wheel - this is to make them easily distinguishable.\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = continent), size = 4)  + \n  geom_point()\n\n\n\n\nWe can use scale_color_brewer and supply one of the palettes mentioned above:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = continent), size = 4)  + \n  geom_point() + \n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nYou can also manually set the colors - for example if you have a specific color scheme which makes sense in the context of your data. To do this, use scale_color_manual\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap, color = continent), size = 4)  + \n  geom_point()+  \n  scale_color_manual(\n    values = c(\"sienna1\", \"sienna4\", \"hotpink1\", \"hotpink4\", \"firebrick\")\n  )"
  },
  {
    "objectID": "data-visualisation-1.html#chart-types",
    "href": "data-visualisation-1.html#chart-types",
    "title": "4  Data Visualisation 1",
    "section": "Chart types",
    "text": "Chart types\nThe geoms we have used are not the only ones available. Ggplot has many more, and there are also packages which extend ggplot with extra geoms to use. Here are some more possibilities:\ngeom_area(), which is a line plot where the y-axis is filled in:\n\nggplot(data = top_movies, aes(x = year, y = total_in_2022_dollars, fill = distributor)) + \n  geom_area()\n\n\n\n\nGeom_boxplot, which visualises distributions:\n\nggplot(data= gapminder_2007_df, aes(x = continent, y = lifeExp, fill = continent), size = 4)  + \n  geom_boxplot()\n\n\n\n\nGeom_smooth, which adds a ‘line of best fit’ to a visualisation, usually used in conjunction with geom_point:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap), size = 4)  + \n  geom_point() + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nOne last useful feature is that we do not have to use the default position scale where the data is mapped linearly to the aesthetics. Another option, for example, is to use a log scale. A log scale is one where each value is exponentially larger than the previous one.\nThis is often useful if your data is unevenly distributed (for example if you have lots of values close to each other, and then some big outliers). Using a log scale can make this easier to read:\n\nggplot(data= gapminder_2007_df, aes(x = lifeExp, y = gdpPercap), size = 4)  + \n  geom_point() + scale_y_log10()"
  },
  {
    "objectID": "data-visualisation-1.html#exercises",
    "href": "data-visualisation-1.html#exercises",
    "title": "4  Data Visualisation 1",
    "section": "Exercises:",
    "text": "Exercises:\nStart a new notebook in your posit workspace, and try the following exercises:\n\nExercise 1:\nAs last week, load the revenue_by_format.tsv file into R. To do this in a new notebook (for it to knit), you’ll need to also load the tidyverse library we’ve used the past few weeks.\n.tsv is another file format, similar to .csv. You’ll need to use the function read_tsv() to do this, in the same way you used read_csv() in earlier weeks.\n\n\nCode\ndf = read_tsv('revenue_by_format.tsv')\n\n\n\n\nExerise 2\nFilter the dataset to only include the format ‘LP/EP’.\n\n\nCode\nfiltered_df = df %&gt;% filter(format == \"LP/EP\")\n\n\nCreate a line plot in ggplot which plots the year on the x axis, and the ‘revenues_in_millions_adjusted’ along the y axis.\n\n\nCode\nggplot(data = filtered_df) + \n  geom_line(aes(x = year, y = revenues_in_millions_adjusted))\n\n\n\n\nExercise 3:\nUsing the correct `scale_, restrict the limits of the plot to the years 2000 to 2020:\n\n\nCode\nggplot(data = filtered_df) + \n  geom_line(aes(x = year, y = revenues_in_millions_adjusted)) + \n  scale_x_continuous(limits = c(2000,2020))\n\n# A better way to do this would be to filter the dataset\n# As you can see, this just removes data outside the years but does not adjust the y-axis scale.\n\n\n\n\nExercise 4:\nSet the breaks of the plot to every 2 years, starting from 2000.\n\n\nCode\nggplot(data = filtered_df) + \n  geom_line(aes(x = year, y = revenues_in_millions_adjusted)) + \n  scale_x_continuous(limits = c(2000,2020), breaks = seq(2000, 2020, 2)) \n\n\n\n\nExercise 5:\nGo back to the original data. This time, draw a line chart which separately charts each category.\n(Note that to do this, you’ll need to summarise the data by category and year, because there is more than one format for each category.)\nSet the color of the line to the category.\n\n\nCode\ndf_sum = df %&gt;% group_by(category, year) %&gt;% summarise(n = sum(revenues_in_millions_adjusted))\n\nggplot(data = df_sum) + \n  geom_line(aes(x = year, y = n, color = category))\n\n\n\n\nExercise 6\nChange the color scale of this chart to something else. From the examples above, pick the appropriate scale_color_ function, and specify a set of colors.\n\n\nCode\ndf_sum = df %&gt;% group_by(category, year) %&gt;% summarise(n = sum(revenues_in_millions_adjusted))\n\nggplot(data = df_sum) + \n  geom_line(aes(x = year, y = n, color = category)) + \n  scale_color_manual(values = c(\"hotpink\", \"lightblue\", \"orange\", \"purple\", \"#F07070\", \"#A16166\"))\n\n\n\n\nExercise 7\nMake some changes to the color legend. First, reverse the order. Next, set the width of the bar to 4cm.\n\n\nCode\ndf_sum = df %&gt;% group_by(category, year) %&gt;% summarise(n = sum(revenues_in_millions_adjusted))\n\nggplot(data = df_sum) + \n  geom_line(aes(x = year, y = n, color = category),size = 2) + \n  scale_color_manual(values = c(\"hotpink\", \"lightblue\", \"orange\", \"purple\", \"#F07070\", \"#A16166\"))+\n  guides(colour = guide_legend(reverse = TRUE)) \n\n\n\n\nExercise 8\nAgain, return to the full dataset. This time, make a count of the total in revenue_millions_adjusted, not by year.\n\n\nCode\ndf_sum = df %&gt;% group_by(category) %&gt;% \n  summarise(n = sum(revenues_in_millions_adjusted))\n\n\n\n\nExercise 9\nCreate a bar chart from this information, using geom_col. Based on the demonstrated code above, make it more readable by adjusting the axis breaks and the coordinate directions.\n\n\nCode\nggplot(data = df_sum) + \n  geom_col(aes(x = reorder(category,n), y = n )) + \n  coord_flip()\n\n\n\n\nExercise 10\nColor the bars in this chart by a continuous variable, revenue_millions_adjusted.\nChange the color scale to a gradient with two colours of your choosing.\n\n\nCode\nggplot(data = df_sum) + \n  geom_col(aes(x = reorder(category,n), y = n, fill = n )) + \n  coord_flip() + \n  scale_fill_gradient(low = 'hotpink', high = 'forestgreen')\n\n\n\n\nExercise 11\nFor the final exercise, you’re tasked with creating a visualisation from scratch in pairs, using a dataset of your choosing.\nThink about which chart type is most appropriate, use the correct geom, and write your reason in the markdown file.\nThe chart should have at least:\n\nAdjusted position limits and breaks (for example highlighting a certain area or ‘zooming in’)\nA custom colour palette including specified breaks"
  },
  {
    "objectID": "data-visualisation-2.html#exercises",
    "href": "data-visualisation-2.html#exercises",
    "title": "5  Data Visualisation 2",
    "section": "Exercises",
    "text": "Exercises\nStart a new .Rmd file in Posit Cloud to do the following exercises:\n\nExercise 1.1\nYou have been asked to work on a historical data visualization project as part of a research project into Irish immigration in the nineteenth century.\nThe project will use data on the Bellevue Almshouse in New York, where poor, sick, homeless and otherwise marginalised people were often sent (both voluntarily and sometimes forcibly).\nThis data was transcribed from the original records by Anelise Shrout. It has been made available as a .csv file on a code repository, Github. To load this data into R, you can do so directly from the internet without having to download it first, using the following code cell.\n\nlibrary(tidyverse)\n\nbellevue_df = read_csv('https://raw.githubusercontent.com/laurenfklein/QTM340-Fall21/main/corpora/bellevue_almshouse_modified.csv')\n\nbellevue_df = bellevue_df %&gt;% mutate(month = cut(date_in, 'month')) %&gt;% mutate(month = lubridate::ymd(month))\n\nI have also made some small changes: I’ve made a new column containing the month of admission, rather than the day. In the final mutate, I have specified that R should read the data as a date.\nYour first task is to make a few simple counts. You don’t need to save these as new datasets - simply write the code which produces the count of the relevant data. As a hint, think about group_by and summarize as being helpful here. The counts needed are:\n\nA simple count of each gender\nThe average age by gender. You’ll also see there are some ‘NA’ values both in age and gender. In order to deal with these, you’ll need to use the code na.rm = TRUE in the appropriate function.\nThe number of admissions per month\nThe number of admissions per profession, per month.\n\nFinally, create a line chart which first filters to include only the professions: c('laborer', 'married', 'spinster', 'widow', 'shoemaker') , and charts the number of admissions by profession with a separate line, per month. Hint: In order to correctly display separate lines correctly, you’ll probably need to specify both color = and group = within the aes()."
  },
  {
    "objectID": "data-visualisation-2.html#exercise-1.2",
    "href": "data-visualisation-2.html#exercise-1.2",
    "title": "5  Data Visualisation 2",
    "section": "Exercise 1.2",
    "text": "Exercise 1.2\nAfter submitting this to the project manager, they have come back to you with a few extra requests:\n\nAdjust the linewidthof the lines to something more easily readable.\nPick a different color scheme, one which specifically takes into account color vision deficiency (CVD).\n\nFor the latter, you’ll need to set the colors manually and use an appropriate scale_color_ function. You can find some appropriate color palettes on this page: http://www.cookbook-r.com/Graphs/Colors_(ggplot2), including some hints on how to use them. Note that with the correct scale, you can specify more colors than you have categories, and it will just use the first ones.\n\nExercise 2.1:\nYou have been asked to review and edit some data visualisations submitted as part of a project looking at incarceration rates in the US.\nSpecifically, you have been asked to look for ethical or accessibility issues with some plots which have been created. In order to ‘fix’ them, you can copy and paste the existing code into a new cell, and make adjustments.\nIn this first visualisation, the creators have attempted to show the differences between white and black incarceration rates in California in the most recent year with relatively fully available data, 2015.\nThe researchers calculated the rates of prison population per head of population, and also calculated the differences between the two categories of prisoner. This has been mapped using a choropleth map (which we’ll learn more about in the coming weeks).\nRunning the following code in order should produce this plot from scratch:\n\nlibrary(sf)\n\nus_prison_pop = read_csv('https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/us-prison-pop.csv')\n\n\nus_state_abb = read_csv('https://raw.githubusercontent.com/jasonong/List-of-US-States/master/states.csv')\n\nbasemap =  st_as_sf(maps::map(\"county\", plot = FALSE, fill = TRUE))\n\nus_prison_pop = us_prison_pop %&gt;% left_join(us_state_abb, by = c('state' = 'Abbreviation'))\n\nus_prison_pop2 = us_prison_pop %&gt;% \n  mutate(State = tolower(State)) %&gt;% \n  mutate(county_name = tolower(county_name)) %&gt;% \n  mutate(county_name = str_remove(county_name, \"county\"))%&gt;%\n  mutate(county_name = trimws(county_name, which = 'both')) %&gt;% \n  mutate(ID = paste0(State, \",\", county_name))  %&gt;% \n  mutate(a = white_male_prison_pop/white_pop_15to64) %&gt;% \n  mutate(b = black_male_prison_pop/black_pop_15to64) %&gt;%\n  mutate(diff = b - a) %&gt;% \n  select(ID, year,a,b,diff,State)\n\nprison_sf = basemap %&gt;% filter(str_detect(ID, \"california\")) %&gt;%\n  left_join(us_prison_pop2 %&gt;% \n  filter(year == 2015) %&gt;% filter(!is.na(year)))\n\nThis final cell is the visualisation itself. Once you have run all the other code, you can copy this one into a new cell and make adjustments.\n\nggplot() + \n  geom_sf(data = prison_sf, aes(fill = diff)) + \n  theme_void() +\n  scale_fill_gradient(low = 'red', high = 'yellow')\n\n\n\n\nYou should comment on the chosen color scheme: what could be improved and why? Make changes to a new version of the code with a more appropriate color scheme.\n\n\nExercise 2.2\nIn this second example, the total male and female prison population has been plotted. In this case, there are a number of issues you might raise regarding the chart. Hint: as well as color schemes, look at and maybe even change the processing/filtering steps leading up to the chart. Do you think these are a fair representation of the trends in the data?\n\nus_prison_pop %&gt;% \n # filter(State %in% c('California')) %&gt;% \n  select(State, year, male_prison_pop, female_prison_pop) %&gt;% pivot_longer(cols = 3:4) %&gt;% \n  group_by(year, name) %&gt;% \n  summarise(n = sum(value, na.rm = TRUE)) %&gt;%  \n  filter(year %in% 2005:2015) %&gt;% \n  ggplot() + \n  geom_line(aes(x = year, y = n, color = name), size = 2, alpha = .5) + \n  scale_color_manual(values =c('pink', 'blue')) + theme_bw() + labs(title = \"Total prison population US, 2005 - 2015\")"
  },
  {
    "objectID": "digital-mapping-1.html",
    "href": "digital-mapping-1.html",
    "title": "6  Digital mapping 1",
    "section": "",
    "text": "7 Creating maps in R\nIn this context, ‘from scratch’ means that on this course we will learn how to create both the underlying map ‘canvas’, as well as the data points or polygons to go on top of it.\nThis week, we’ll practise making the ‘basemaps’ onto which further data will be drawn later. We’ll also learn how to visualise some additional data as choropleth maps.\nIt’s worth understanding a little of what happens when we want to make a map of shapes.\nWhen we want to create maps with computers, using a Geographic Information System, or GIS, the computer needs to know what to map. Essentially, it needs some data which tell it how to draw shapes. This is usually in the form of a list of coordinates, which make up a polygon. Each individual polygon (or set of polygons, to show separate islands and so forth) might represent a country, region, or municipality, such as this one of these of the country borders of Spain and the Netherlands:\nBecause this is vector data, we can read it into R and make changes to it, for example adjusting the ‘fill’ color, and changing the color and thickness of the border lines."
  },
  {
    "objectID": "digital-mapping-1.html#overview",
    "href": "digital-mapping-1.html#overview",
    "title": "6  Digital mapping 1",
    "section": "Overview",
    "text": "Overview\nIn this part of the course, we’ll learn how to create data maps. By data maps, I mean maps in which geographic elements, such as points, lines, and polygons, which are mapped to external data, for instance cultural or political data. These kinds of maps have become very familiar and widely used in digital history, data journalism and infographics in recent years.\nMaps are useful because some data or information has inherently spatial aspects - where the data is from is as important as what. We might want to understand geographic patterns of a phenomenon, for instance how language is used in different regions, or the demographic and economic patterns of particular places, and so forth.\nOn benefit of a map is that they can be useful cognitive shortcuts. Because we all have models of maps in our head already (think about how you know certain pieces of information about the borders of the provinces or positions of towns in your home country), we can often grasp very quickly lots of information. If you know the rough positions of the towns and provinces of the Netherlands, for instance, you will probably be able to understand some data about them more quickly than if the same information was presented to you in the form of a list or table.\nData maps also help us to see patterns which are specifically geographic in nature - we might see, for example, that a certain data point is more common on coasts, or in particular regions, or notice differences between the centre and periphery."
  },
  {
    "objectID": "digital-mapping-1.html#creating-digital-maps",
    "href": "digital-mapping-1.html#creating-digital-maps",
    "title": "6  Digital mapping 1",
    "section": "Creating digital maps",
    "text": "Creating digital maps\nIn the maps lecture given by Martijn Storms, we saw maps which had mostly been created by hand. Digital mapping has its own set of techniques, as there are different methods by which we can represent geographic and physical features of the ‘real’ world as computer code.\nThe two most common formats for doing this are called raster maps and vector maps.A raster map represents the physical elements as individual pixels. This is commonly used when we digitise a physical map - each pixel on a map will be shaded and coloured differently, and this shade will mean something. Another example of a raster map is an elevation map. Each pixel is colored by the elevation of that point, i.e.\n\nThis kind of map can be read by a computer, to calculate things like travel times, or to understand ecological differences, and so forth.\nThe other kind of map is called a vector map. A vector map represents geographic elements as a series of numerical values. These geographic elements are generally points, lines, or polygons. For instance, a point is represented by a set of two coordinates, giving the latitude and longitude on earth. A line would be represented by a series of numbers, which when joined together make up a line on a map.\nThe main difference between the two is that vector maps contain shapes which can be specifically read by computer code. Unlike a raster map, with vector data, we can outline the shape of a river or road, or the outline of a province or other political border. This makes vector data particularly useful for creating data visualisations."
  },
  {
    "objectID": "digital-mapping-1.html#mapping-aesthetics",
    "href": "digital-mapping-1.html#mapping-aesthetics",
    "title": "6  Digital mapping 1",
    "section": "Mapping Aesthetics",
    "text": "Mapping Aesthetics\nAs with all data visualisation, the process involves mapping aesthetics to values. The key difference between maps and other data visualisations is that position is usually based on geographic position, i.e. areal physical location on earth. The most common kinds of data maps you’ll find will use position, size, color, and shape to draw points on a map, or will use position and color (and perhaps patterns) to fill in polygons.\nWhich is used is usually dependent on the specific kind of geographic data you have. Points ‘point’ to specific, exact places on a map, for instance a town, village, or even an exact address. Polygons are widely used when the data we have relates to political or cultural regions and borders.\n\nPoints Map\nThis first graphic is an example of simply using position to create a data visualisation. Each covid death has been drawn as a single tiny black point, and through the placement of millions of these points, we can see precisely where the death toll from Covid was highest.\n\nJust like the earlier data visualisations we made on this course, we can also map further aesthetics as well as position to the data. In this next example from the New York Times, data on weather records was mapped. The authors drew points using both colour (a categorical colour scale, either red for heat or blue for cold records) and size (the amount by which the previous record was broken) to communicate the areas where temperature records had been most extreme in that year.\n\n\n\nChoropleth maps\nA classic use of mapping data to polygons are election maps. This is because election results are usually counted by region, municipality, or other political border. In this example from the 2011 election in the Netherlands, each municipality is represented by a polygon. Each polygon is then coloured by the winning party. A discrete colour scale is used, because this is categorical data (each region has exactly one winning party).\n\nA map where colour is used to present statistical data through polygon shapes is known as a choropleth map. These maps tend to be useful when we have data which is not related to a single point (such as a town or city), but is connected to regions, such as countries, municipalities and so forth.\nHowever, there are some issues we should be aware of making choropleth maps. In the example of the election map above, about half of the map is coloured blue. This might lead us to think that the VVD party received over half of the total votes. But while they were the largest party, in fact they won 112 seats out of a total of 566 - about one-fifth of the votes.\nThis is of course because this map hides the fact that elections are related to population rather than land area. The VVD party are bigger in rural areas which tend to be sparsely populated. Generally, when creating data visualizations, it is best practice that each pixel or piece of ‘ink’ on the page represents the same amount of numerical value. This is not the case if we use geographic data like this. In this case, the numerical unit is a vote, and some votes are represented by less area than others, because of population density.\nOne fix for this is to create a cartogram map. This attempts to distort a map so that the area better reflects the data. This example distorts the shapes of the countries of the European Union so that the area of each is related to the amount of the budget they either contribute or receive (between 2007 and 2013, and per capita)."
  },
  {
    "objectID": "digital-mapping-1.html#working-with-maps-the-sf-package",
    "href": "digital-mapping-1.html#working-with-maps-the-sf-package",
    "title": "6  Digital mapping 1",
    "section": "Working with maps: the sf package",
    "text": "Working with maps: the sf package\nTo build a map, we need to import special data into R. To do this we’ll use a package called sf. sf stands for simple features. It is a data format specially made for geographic data. Essentially, a simple features object looks like the dataframes (the rows and columns) we have been working with all along, except with one new column, called geometry. Each row of the data can be considered a ‘feature’, meaning a single point, line, or polygon.\nSo each row contains a single feature. Usually, these will have additional information about them. For example, one ‘feature’ might be a polygon shape for a single country. This might have a row of information with the country name, ISO code, even population and so forth.\nThe special ‘geometry’ column is what makes it geographic information. This contains the geographic information for that feature. In the case of a polygon shape for a country, it will contain a list of the points which, joined together, make up the shape of that polygon. A points feature will contain a single set of geographic coordinates, and a line will contain a list of points, but it won’t be joined-up, like a polygon. Each sf object will only contain one type of feature.\nAs well as this geometry column, each sf object will have a further piece of information attached to it - a CRS or coordinate reference system. Often, if you download a map file from the internet, it may already have one of these embedded. If you make your own set of features (a set of points, for example), then you will need to add a CRS yourself. In order to add maps together (for example, to have a baselayer of polygons and a layer of points over that), they’ll both need to have the same CRS. We’ll learn how to add this later.\nThe good news about sf objects is that they behave just like dataframes. This means that we can apply all the techniques and code we learned over the past few weeks to these objects. For example, if you have a sf object which is a list of countries and you want to map only a few of them, you could do this with filter(). Or, you can use group_by() and summarise() if you have a group of points and you want to group them all together.\nsf also has many additional features to to complicated calculations and transformations of geographic data. If you are interested in learning more, I highly recommend reading all or part of Geocomputation with R. For now, we will stick with simple calculations, based on the kinds of things we have already been learning, just with geographic data."
  },
  {
    "objectID": "digital-mapping-1.html#basemaps",
    "href": "digital-mapping-1.html#basemaps",
    "title": "6  Digital mapping 1",
    "section": "Basemaps",
    "text": "Basemaps\nTo begin most maps, you’ll need a basemap. This is often going to be a simple map of the world, allowing readers to properly orientate themselves - so they’re not looking just at a group of points or polygons floating in space, for example.\nIn R, the easiest way to create a basemap is using a package and a database called RNaturalEarth. RNaturalEarth allows you to connect to a database containing many different shapefiles, that is, datasets containing shape information. This includes many different borders, such as state, country, municipality etc. Depending on your needs, it may be helpful to have some of these.\nIn other cases, you’ll just want a very simple map of the world. This is particularly the case with historical maps, because boundaries change frequently. It may not make sense to have a border of modern Germany if your historical data comes from the eighteenth century, for example. In this case, you’ll want to map coastlines instead of countries, or, you can colour your basemap so that the internal borders are the same as the fill color.\nAs well as basic shapes of countries, you can download rivers, lakes, and other physical features.\nTo use sf and Rnaturalearth, you’ll first need to install them, if you haven’t already. It should already be working in your Posit cloud instance, but if not, you can ‘uncomment’ the following code by removing the # from the beginning, and then run the cell:\n\n# install.packages('rnaturalearth')\n# install.packages('sf')\n\nNext, you would load the library:\n\nlibrary(rnaturalearth)\nlibrary(sf)\n\nsf_use_s2(FALSE) # this last line is a fix for some problems we might encounter later...\n\nSpherical geometry (s2) switched off\n\n\nThe package contains three ‘standard’ maps which can be used.\nThe first is a dataset of country borders. to load this, you’ll need to use the function ne_countries(), as well as specify a few more options:\nscale = , which will determine how detailed the borders should be, one of small, medium, or large.\nreturnclass =, which will specify the type of data to be downloaded from the database. In all cases, we’ll use returnclass = 'sf' (note the quotation marks around sf).\nOptionally, you can specify that the function return a single country or list of countries. This is done using the syntax country = 'Netherlands' or country = c('Netherlands', 'Spain').\nThe following cell will download a dataset in sf format, of all world countries:\n\nworldmap = ne_countries(scale = 'medium', returnclass = 'sf')\n\nThis will download a map of just Germany:\n\ngermanymap = ne_countries(scale = 'medium', returnclass = 'sf', country = 'Germany')\n\nOr a map of Germany and the Netherlands:\n\nde_nl_map = ne_countries(scale = 'medium', returnclass = 'sf', country = c('Germany', 'Netherlands'))\n\nThe next useful map type is coastlines. This downloads a simple map of the world’s coastlines, useful if you want a map without any political borders. This is done with the function ne_coastline. In this case, we should specify the scale and returnclass:\n\nworldcoastlines = ne_coastline(scale = 'medium', returnclass = 'sf')\n\nOnce this has downloaded, open the new ‘worldmap’ object in your environment to take a look at it. You’ll see it is a dataframe with a large number of columns, with one row for each country. If you go to the very last column, you’ll see it has a ‘geometry’ column, where it stores the shapes for each country. But as well as this basic information on the shapes, the data contains continent and even some economic and demographic information.\nUsing filter just as we have in previous weeks, you could easily filter this map to only include a certain continent or certain development index, or perhaps only countries with a population greater or smaller than a specific number. Here are a few examples:\nCreate a map of Europe:\n\neuropemap = worldmap %&gt;%\n  filter(continent == 'Europe')\n\nCreate a map of counties with a population of less than 5 million:\n\nsmall_countries = worldmap %&gt;% \n  filter(pop_est &lt; 5000000)"
  },
  {
    "objectID": "digital-mapping-1.html#visualising-with-ggplot",
    "href": "digital-mapping-1.html#visualising-with-ggplot",
    "title": "6  Digital mapping 1",
    "section": "Visualising with ggplot",
    "text": "Visualising with ggplot\nPlotting and visualising these maps will also be done with a package you are familiar with: ggplot2.\nTo draw maps instead of regular data, it just requires some small changes. To draw a map we’ll use a function starting with geom_, as we did with geom_col, geom_point and so forth.\nThe geom for maps is called geom_sf() and it works in the exact same way as the others, except it knows to treat the data as a map.\nUsing geom_sf on its own on an sf object will take that object and plot it as a simple two-dimensional map. Following this, you can use the rest of the ggplot syntax you learned to modify the maps. For example, you can change the fill and color of the shapes, and the size of points, using fill = , color = and size =. These can be mapped to specific parts of the data if you include them within aes(), as before.\nYou can also use the same code as before to change the scales, add annotations, and set limits to the plot.\nTo create a basic map from an sf object simply use ggplot() as before, but add + geom_sf() to your map:\n\nggplot() + geom_sf(data = worldmap) \n\n\n\n\nYou can use filter to limit the map to a single continent or region.\n\nworldmap %&gt;% \n  filter(continent == 'Europe') %&gt;% \nggplot() + geom_sf() \n\n\n\n\n\nworldmap %&gt;% \n  filter(sovereignt == 'Netherlands') %&gt;% \nggplot() + geom_sf() \n\n\n\n\n\nSetting limits\nUsing filter might not always give satisfactory results. In the above case, it includes places which are politically at least part Europe, such as overseas territories and the entirety of Russia. In the case of the Netherlands, this makes the main part of the map very small and difficult to see(if we wanted to include this place, it may be best to add an inset, but that will have to be left to another lesson).\nIf our data was more specifically about western Europe, we might want to pick a custom ‘square’ of the map to show. We can do this using the code + coord_sf(). This should be added after the rest of the map code.\nWithin coord_sf, we’ll set the start and end limits for the x and y axis (or longitude and latitude coordinates), using xlim and ylim, like this:\n\nworldmap %&gt;% \nggplot() + \n  geom_sf() + \n  coord_sf(xlim = c(-10, 25), ylim = c(38, 60))\n\n\n\n\nA map can consist of many layers of geographic information stacked on top of each other.\nAs well as the basic countries and coastline maps, it is also possible to download lots of other specific geographic data from RNaturalearth, using a function ne_download. To use this, you’ll need to specify the category and type of the map, as well as the scale and returnclass as before. The full list of categories and types is available on this page, towards the end.\nThe following example will download the ‘rivers_lake_centerlines’ data from the ‘physical’ category of maps:\n\nrivers = rnaturalearth::ne_download(scale = 'medium',category = 'physical', type = 'rivers_lake_centerlines', returnclass = 'sf')\n\nWe can add this river data using a new layer, with another geom_sf and a +.\nOne thing to note is that you always need to put the coord_sf() after your last geom_sf, otherwise it will ignore it and give the full map:\n\nworldmap %&gt;% \nggplot() + geom_sf() + \n  geom_sf(data = rivers) + \n  coord_sf(xlim = c(-10, 25), ylim = c(38, 60))\n\n\n\n\nWe can also change aspects of the map such as the fill of the shapes, and the colour of the outline. As with before, this is done using color = and fill =. To change all of the geographic data of a layer to a single color, simply type it within the geom_sf . If it is to be mapped to specific data, it should be enclosed within aes()\n\nworldmap %&gt;% \nggplot() + geom_sf(color = 'black', fill = 'lightgreen') + \n  geom_sf(data = rivers, color = 'blue') + \n  coord_sf(xlim = c(-10, 25), ylim = c(38, 60))"
  },
  {
    "objectID": "digital-mapping-1.html#geographic-data",
    "href": "digital-mapping-1.html#geographic-data",
    "title": "6  Digital mapping 1",
    "section": "Geographic data",
    "text": "Geographic data\nThese maps are, essentially, just reflections of physical geography. To make a map into a data visualisation, we need to actually map some additional data on top of this, to specific aesthetics such as size and color. As mentioned above, the most common ways to do this are either choropleth or points maps. We’ll work with the latter next week.\nThe country shapefile downloaded from RnaturalEarth has some basic data attached to it, which we can use to map. If you look at the worldmap object in your environment, you can see how the data has been stored in columns.\nIn your own maps, you probably won’t have data directly attached to the polygon object.\nHowever the Rnaturalearth data is useful to practise the syntax of map data visualisations.\nIn the RNaturalEarth data, a population count for each country is stored in the column pop_est. We can use this data field to color the polygons, just as we have with data in earlier classes.\nAs before, add %&gt;% ggplot() and geom_sf() to the plot. Next, use aes() and then specify fill = pop_est, within the aes().\n\nworldmap %&gt;% ggplot() + geom_sf(aes(fill =pop_est))\n\n\n\n\nRNaturalEarth also includes categorical information. The object also includes a column economy, which assigns each country a discrete label depending on the economy type. Let’s try mapping this:\n\nworldmap %&gt;% ggplot() + geom_sf(aes(fill =economy))"
  },
  {
    "objectID": "digital-mapping-1.html#adding-geographic-data",
    "href": "digital-mapping-1.html#adding-geographic-data",
    "title": "6  Digital mapping 1",
    "section": "Adding geographic data",
    "text": "Adding geographic data\nAs mentioned, usually you will want to bring external data to your maps rather than use this basic information. To do this, we’ll go back to another function we learned earlier in the course - joins.\nAs an example, take this dataset containing a count of the refugees which arrived in the US in 2015, by place of origin. You can load this with the code below:\n\nrefugees_2015_us = read_csv('refugee_us_2015.csv')\n\nRows: 113 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): origin\ndbl (1): n\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you look at this object, you’ll see it contains two columns: the origin country, in a column called ‘origin’, and a count of the number of arrivals, in a column called ‘n’.\nUsing left_join , we can merge this with the worldmap data, so that it contains an additional column, n, with the refugee arrivals information. To do this, we need to specify that the origin column from the arrivals data is matched with the name column in the worldmap data:\n\nworldmap_with_refugee = worldmap %&gt;% \n  left_join(refugees_2015_us, by = c('name' = 'origin'))\n\nNote that because of the complexities of country names, it may not match all up correctly.\nLastly, we can map this data, specifying that ggplot should map the new n columnto the fill aesthetic:\n\nworldmap_with_refugee %&gt;% \n  ggplot() + geom_sf(aes(fill = n))"
  },
  {
    "objectID": "digital-mapping-1.html#exercises",
    "href": "digital-mapping-1.html#exercises",
    "title": "6  Digital mapping 1",
    "section": "Exercises",
    "text": "Exercises\n\nDownloading\n\nIn each case, make sure you save the map to the environment, by giving each object a name using &lt;objectname&gt; =\nUsing rnaturalearth, download a full map of the countries of the world, at a medium scale.\nDownload a new map of a region of your choice, using either continent, region_un, region_wb, or subregion.\nDownload a new map using the function ne_download(). The category should be ‘cultural’, and the type should ‘urban_areas’.\n\n\n\nVisualising\n\nUsing ggplot, make a basic plot of the region map.\nNext, restrict this map to a smaller area, using coord_sf\nChange the fill of the polygons to a new color.\nAdd the urban areas map as a second layer to your regions.\nSet the fill of the polygons in the region map to the pop_est column. Set the urban areas to a new color.\nChange the fill scale of this map (for example using scale_fill_viridis_c()\n\n\n\nAdding data (harder difficulty!)\nThe code cell below will download a dataset containing data on all past Nobel prize winners. Run this (or copy and paste into the notebook you’re using) to load it into the environment.\n\nnobel_df = read_csv('https://raw.githubusercontent.com/melaniewalsh/Intro-Cultural-Analytics/master/book/data/nobel-prize-winners/nobel-prize-winners.csv')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 956 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (19): name, bornCountry_original, bornCountry_now, bornCity_original, b...\ndbl   (5): bornLong, bornLat, diedLong, diedLat, year\ndate  (2): born, died\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nGet a simple count of the number of winners for each country (save it as an object such as nobel_count_df)\nMerge this new count dataset with the world map (or region map) you created above, using left_join.\nCreate a new map which visualises this information. Experiment with coord_sf and scale_fill_ to make the map more readable.\n\n\n\nTough exercises\n\nCreate a map which colors countries by the number of female Nobel winners by head of population. To do this you’ll need to use filter, mutate, along with the pop_est column from the countries map data.\nMake a visualisation which gives a series of small maps, one for each category of Nobel prize, where each country is colored by the total number of winners. You’ll need to use a function called facet_wrap(), which will be added to the end of your plot using +. You’ll need to do some independent work to find out the correct syntax for this!\n\n\n\nTougher exercises\nThe sf package can do some complicated geographic calculations. If you’re interested, you could try out some of the following. In order to do these, you’ll need to run sf_use_s2(FALSE) and make sure the package lwgeom is installed (this should already be done for you in Posit, but bear in mind in case you get errors).\nRestrict the worldmap object to only countries which are smaller than 100 million square metres. You’ll need to make use of mutate() and st_area(), and then filter().\nBy default, st_area will return the answer in units of metres square. If you want to filter using a simple number, you need to ‘force’ R to treat the area as a number using as.numeric()\nCreate a map containing only Germany and countries which border it. To do this, you’ll need to make object containing only a map of Germany, and then use st_intersection on both the full map and the new Germany map.\nVisualise this new map using some of the worldmap data!"
  },
  {
    "objectID": "digital-mapping-2.html#steps-to-making-a-map",
    "href": "digital-mapping-2.html#steps-to-making-a-map",
    "title": "7  Digital Mapping 2",
    "section": "9 Steps to Making a Map",
    "text": "9 Steps to Making a Map\n\nImport your geographic data\nFigure out the geographic unit\nFind a base map which corresponds\nImport your base map\nFind the common field\nSummarise your data\nJoin your data and the map together\nMap with ggplot and sf\nFinishing touches\n\n\n1: Import your geographic data\nFind the ‘raw’ dataset you want to use. This will of course have some kind of geographic information, such as country, or state, or region, as well as further information. As an example, this is a dataset of Nobel prize winners\n\n\n2: Figure out the Geographic Unit\nEach row of information in your data will have a geographic unit - the type of area on the map where it records the geographic information. These are usually based on some kind of political or geographic border. Examples include nations, regions, provinces, municipalities, election areas, and so forth. This will be recorded in a single column.\nYour data should only have one type of geographic unit. In other words, you shouldn’t have data which mixes more than one together. For example, if you had a ‘place’ column, it won’t work if sometimes it records the country, and sometimes the province. It can have different geographic units in different columns, for example one column with the country. and another with the province. If this is the case, decide which level you want to use.\n\n\n3: Find a base map which corresponds to this geographic unit.\nYou need to find a map containing features which correspond to the geographic unit you are going to use in your data. If you have a dataset where information is recorded on the country level, you’ll need a world map in which each feature is a country (as the one we used last week from R Natural Earth.\nIf you have more specific geographic data, you’ll probably need to find a corresponding ‘shape file’. These are often released by official agencies and can be found through a search engine. For example, Dutch geo data can often be found through https://www.pdok.nl/. Shapefiles for EU regions can be found at https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts. These use a set of boundaries called NUTS, which range from NUTS-0 (countries) to NUTS-3 (small regions).\nRemember that adminstrative units are often redrawn, so try to make sure that your data and map match as much as possible.\nIn some cases, these shapefiles will contain all possible geographic units in one dataset. In that case, you’ll need to filter to only include the relevant ones.\n\n\n4: Import your base map into R\nUsing sf, import your basemap. Usually this can be done by using st_read, and giving the location of the shapefile.\n\n\n5: Find a Common Field\nNow, you need to look at your two datasets - your data and your shapefile - and figure out what field they have in common. This means, you need to find the column in each one which uses a standardised set of names.\nA simple example is where both have a column with country names.\nA good practice is to use a standardised code where one is available. For instance, both dataset may have a standardised ‘ISO’ code where the country is recorded. This is less ambiguous than a country name, which may have slight variants, or be recorded in another languages, etc.\nIn the case of smaller regions such as EU NUTS regions, you will likely have to use a code rather than a specific name.\n\n\n6: Summarise your Data\nOnce you have your common code, you can get your dataset ready. Usually, this will mean doing a summary (such as a count, or maybe getting an average, or perhaps some kind of proportion by population) for each entry in the geographic field you are using. For example, if the field in common is an EU NUTS region code, then you would do a summary for each NUTS region.\nOne exception to this is if you want to draw multiple maps, one for each category. In this case, you might summarise by two variables: the category, and the region. Later, you’ll need to use facet_wrap to draw each category as a separate map.\nAt this stage, you may also want to filter your data. This might be to restrict the data to a certain year or timeframe. You will also need to filter your data if you have multiple geographic units in the same column. For instance, your data may have a ‘region’ column, where it records not just NUTS level 2, but also NUTS level 1 and 3. In this case you need to figure out how to restrict it to a single geographic unit.\n\n\n7: Join your data and map together.\nUsing left_join, you will now merge together the map with the summarised dataset. If the column names are not the same, you’ll have to specify which ones it should use (see previous lesson for this).\n\n\n8: Map with ggplot and geom_sf\nNow, you can make a map! Remember to use aes() and fill = to specify how ggplot should colour your data.\n\n\n9: Finishing Touches\nNow, make edits to your map to make it more readable and looking better aesthetically. You’ll probably want to adjust the limits of the map using coord_df, and experiment with the scale using scale_fill_. You could also think about binning the colors (often it’s easier to interpret specific colors on maps rather than a continuous scale). This is the point where you will need to use facet_wrap() to draw maps separately, if you went down this route.\nYou could also add a map scale bar and a compass using the package ggsn (you’ll need to install it separately using install.packages."
  },
  {
    "objectID": "digital-mapping-2.html#exercise",
    "href": "digital-mapping-2.html#exercise",
    "title": "7  Digital Mapping 2",
    "section": "Exercise",
    "text": "Exercise\nIn groups, create a data map from scratch by following the steps above.\nUse one of the following data sources. You’ll need to summarise the data first of all, and think about what it is exactly you want your map to show. You’ll also have to do some independent research to find a suitable shapefile. Once you have done this, you can read it into R using the function st_read(). I can help with any issues at this step!\n\nNobel prize data. This dataset we worked with last week. It contains country and place of birth and death for Nobel prize winners, as well as other information. It’s available here, or you can look up the code we used to read it directly into R last week.\nThis dataset of data on dog adoptions in the US. It was originally used for this story on the news website The Pudding. The easiest way to load this into R is to directly use the ‘raw’ link on read_csv in R. You can also download the file, and then upload it to Posit, and use it that way. Learning how to use datasets directly from Github is a very useful thing to know!\nEU statistics, from the official eurostat website. There are many datasets here. To find one which is suitable for creating a data map, look for where the geographic unit (the NUTS code) has been specified in the description.\nUS Refugee arrival data, from here. Again, you’ll need to figure out how to download this from Github.\nUNHCR refugee data. The easiest way to access this is through the R package refugees. This can be installed using install.packages('refugees') . Further information on using this package can be found here.\nTrans-Atlantic Slave Trade data. Available here. Again, from Github, and easiest to load by reading the ‘raw’ file directly into R using read_csv()\nDutch war monuments dataset (from Wikipedia). In this case, I have provided both the dataset and the shapefile, available in the Posit workspace."
  },
  {
    "objectID": "digital-mapping-3.html#getting-points-for-maps",
    "href": "digital-mapping-3.html#getting-points-for-maps",
    "title": "8  Digital Mapping 3",
    "section": "Getting Points for Maps",
    "text": "Getting Points for Maps\nThere are many ways to get sets of points to map. In some cases, you might be given the coordinates alongside the data itself. In many other cases, you might just have a city, village, or address name with no coordinates. In this case, you’ll have to find coordinates for these places.\n\nManual Method\nIf you have a small number of well-known, modern places (a list of capital cities, for example), you might just want to look them up on Wikipedia. Each place should have a hyperlink with a set of coordinates, given in degrees/minutes/seconds. If you click on this link you’ll land on a page which also contains the decimal notation, which can be copied into a spreadsheet, for example.\n\n\nGazetteers\nIf you have a large number of places, maybe including some which are more obscure, are historical, or in several languages, you could use a gazetteer. A gazetteer is like a geographical dictionary: it contains names of places, and usually some kind of reference to their position on earth.\nPerhaps the most well-known gazetteer is Geonames, which contains coordinates and other information for about 12 million place names. Gazetteers are often made with specialist subjects in mind, for example the Pleiades gazetteer of ancient places, or the World Historical Gazetteer.\nMost gazetteers allow you to download the entire underlying dataset, which can be useful to link your dataset of place names to the relevant coordinates.\nGazetteers can be quite complex, because of the huge variety in the way places are given names, particularly over time, and because there are often multiple places with the same name. Places can also be named differently in different cultures and languages. Particularly if you want to automatically link places to coordinates, you’ll need to proceed with caution, and check your results manually afterwards.\n\n\nGeocoding service\nAnother way to find coordinates for a list of places is to use a geocoding service. These are resources which allow you to enter or upload a list of place names or addresses, which are then looked up using a geocoding API, and the coordinates are returned. This is particularly useful if you have a fairly large dataset of places, and if they are modern addresses. An advantage of a geocoding service is that they will usually take a full address into account, which is not always the case with a gazetteer. In this way, they can distinguish between Dublin, Ireland, and Dublin, Ohio.\nThese tools are usually commercial services which allow you to use them for free with some restrictions. One example of a web service is Geoapify. You can also use services in certain versions of QGIS and ArcGIS."
  },
  {
    "objectID": "digital-mapping-3.html#making-points-maps-in-r",
    "href": "digital-mapping-3.html#making-points-maps-in-r",
    "title": "8  Digital Mapping 3",
    "section": "Making points maps in R",
    "text": "Making points maps in R\nWhatever method you use, you’ll need at minimum, a dataset containing latitude and longitude coordinates (in separate columns). In many cases, you’ll also want some further data about each point, which could be used to control their size or colour.\n\nMaking a simple features object\nOnce you have this, you’ll need to make your own simple features object from this dataset. If you remember from last week, a simple features object is a special geographic object, which can be created and edited using the R package sf. The maps we downloaded from RNaturalEarth were in this format.\nLet’s start with the dataset of Nobel prize winners we used last week to make a choropleth map. This data also has longitude and latitude points for each place of birth and death, making it quite easy to turn into a map.\nIn order to turn this into a simple features object, we use a function from the sf package called st_as_sf(). In this function, we will need to specify three things: the dataset of coordinates, the columns containing those coordinates, and the Coordinate Reference System (CRS). Recall from last week that the CRS specifies how distances should be calculated, and which projection should be used.\nFirst, let’s load the dataset and the libraries we need to use.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nmovie_locations = read_csv('movie_locations.csv')\n\nCreate the sf object using the following code:\n\nmovie_locations_sf = st_as_sf(movie_locations, coords = c('lng','lat' ), crs = 4326,na.fail = FALSE)\n\nThe argument coords = needs the names of the longitude and latitude columns, in the correct order. They should be given in a vector (surrounded by c(). Next we specify the Coordinate Reference System using crs =. We’ll use the CRS 4326, which is a very widely used one. Last, we need to specify that it should ignore missing values, with the code na.fail =FALSE. Otherwise, it would give an error if we have any missing coordinates.\n\n\nMap with ggplot and geom_sf\nTurning this into a basic map is very simple, and follows the syntax from previous weeks, using geom_sf.\n\nggplot() + \n  geom_sf(data = movie_locations_sf)\n\n\n\n\nThis map obviously needs some work to make it readable. Most likely, it’ll need a background map of the world to help with orientation. This is done by simply downloading a base map using RNaturalEarth, and adding it as a layer to the code above (add it before the points data so it doesn’t draw on top of it). We may also want to adjust the limits of the coordinates using coord_sf().\n\n\nAggregating the points data\nA slightly more fundamental problem in this case is that it’s not a very accurate visual representation of the data. This is because each place is simply drawn as a point. If there are multiple instances of the same set of coordinates, these will be drawn on top of each other and will disappear.\nIn most cases, we’ll want to aggregate the points information somehow. This is done using group_by() and tally() or summarise(). We need to include the coordinates information in the group_by, and we make the sf object afterwards. Here is how we could get a new dataset which counted the instances of each city of birth (the bornCity_now column in the dataset)\n\nmovie_locations_agg = movie_locations %&gt;% \n  group_by(narrative_locationLabel, lat, lng) %&gt;% \n  tally()\n\nmovie_locations_agg_sf = st_as_sf(movie_locations_agg, coords = c('lng','lat' ), crs = 4326,na.fail = FALSE)\n\nAgain, this can be turned into a map using ggplot and geom_sf(), this time setting the size to the calculated column, which is called n. This needs to be done within aes().\n\nggplot() +\n  geom_sf(data = movie_locations_agg_sf, aes(size = n))\n\n\n\n\n\n\nAdding further variables\nAnother thing we might want to do is to visualise the dots by some additional variable, for example the category of Nobel prize. We make a simple dataset, but add the category variable in to the group_by() function.\n\nmovie_locations_agg = movie_locations %&gt;% \n  group_by(narrative_locationLabel,genreLabel, lat, lng) %&gt;% \n  tally()\n\nmovie_locations_agg_sf = st_as_sf(movie_locations_agg, coords = c('lng','lat' ), crs = 4326,na.fail = FALSE)\n\nWe can specify the color using color = within the aes(), as with size. However, we will also run into the problem of points being invisible because others are drawn on top of them. One trick to fix this is to set the transparency of the points to a value lower than one, using alpha =. In this case, alpha = should not go within the aes(), because we want to set all points to a single value.\n\nggplot() +\n  geom_sf(data = movie_locations_agg_sf, aes(size = n, color = genreLabel), alpha = .5)"
  },
  {
    "objectID": "digital-mapping-3.html#exercise",
    "href": "digital-mapping-3.html#exercise",
    "title": "8  Digital Mapping 3",
    "section": "Exercise:",
    "text": "Exercise:\nMake a points map using the Nobel Prize dataset we worked with last week. Try the following:\n\nAdd a basemap using RNaturalEarth\nRestrict the map to a certain area\nAggregate the data in a way you see fit (for example by gender, category)\n\nYou can load the dataset into your workspace with the following:\n\nnobel_prize = read_csv('https://raw.githubusercontent.com/melaniewalsh/Intro-Cultural-Analytics/master/book/data/nobel-prize-winners/nobel-prize-winners.csv')"
  },
  {
    "objectID": "network-analysis.html#download-and-install-gephi",
    "href": "network-analysis.html#download-and-install-gephi",
    "title": "9  Network Analysis",
    "section": "Download and install Gephi",
    "text": "Download and install Gephi\nThe first step is to download and install the software, which runs on Windows, Linux, or MacOS. Go to https://gephi.org/users/download/ and follow the instructions for your operating system."
  },
  {
    "objectID": "network-analysis.html#download-and-rename-the-data",
    "href": "network-analysis.html#download-and-rename-the-data",
    "title": "9  Network Analysis",
    "section": "Download and rename the data",
    "text": "Download and rename the data\nOnce it is installed, the next step is to get the network data. Remember from the lecture, that a network consists of nodes and edges. Gephi needs at least an edge list, which is a list of all the connections in the network, one row at a time. Because we also want to give extra information about the MPs (the party they are part of, and the position of that party), we will also add a separate node list to Gephi.\nFirst, download the node list and the edge list to your computer and put them somewhere you can find them.\nOpen these two files in a spreadsheet programme such as Excel. Looking first at the edge_list.csv file, you’ll see it has three columns: two columns of MPs (mp1, mp2), representing the IDs for a pair of MPs who co-sponsored at least one bill together, and a third column, nCosponsor, which is a count of the number of times they co-sponsor together.\nThis next step is crucial. In order to recognise the edge list and node list correctly, Gephi expects the columns to have very specific names. The edge list should have the format Source, Target, Weight. With the file open in a spreadsheet programme, change the existing three columns to these names, paying attention to the upper case letters at the beginning. Save the file.\nNext, open the nodes_list.csv file. This needs one change in order to make it compatible with Gephi. In order to link the IDs in the edge list to the IDs here, the first column in the nodes list should be rename to Id, again paying attention to the upper first letter. Make this change and save the file."
  },
  {
    "objectID": "network-analysis.html#open-gephi-and-create-a-new-project",
    "href": "network-analysis.html#open-gephi-and-create-a-new-project",
    "title": "9  Network Analysis",
    "section": "Open Gephi and create a new project",
    "text": "Open Gephi and create a new project\nOnce you’ve done this, open the Gephi application. You’ll be greeted with this screen:\n\nClick on ‘New Project’ to begin with. You’ll notice three buttons near the top of the screen: ‘Overview’, ‘Data laboratory’, and ‘Preview’. Clicking these will switch between different screens. The Overview screen will show a graphical overview of your network, once uploaded. We’ll go back to this one. The Data Laboratory screen is where you’ll upload and change the network data. The Preview screen will show a version of your network ready to export to an image file."
  },
  {
    "objectID": "network-analysis.html#upload-the-node-list-to-gephi",
    "href": "network-analysis.html#upload-the-node-list-to-gephi",
    "title": "9  Network Analysis",
    "section": "Upload the node list to gephi",
    "text": "Upload the node list to gephi\nThe first step is to add the node list, which contains the information about the individual MPs. To upload data to gephi, switch to the ‘data laboratory’ view, and click the ‘import spreadsheet’ button.\n\nOn the next screen (this will look a little different on Mac), go to the folder where you saved the two files, and open node_list.csv.\nOn the next screen, change the import type to ‘nodes table’, and click Next:\n\nHere, you need to make two small adjustments before you move onto the next stage. The data left_right and eu_anti_pro are a number, from 0 to 10, measuring where on the political spectrum the party of this MP falls in these ways (0 is most left-wing and 10 most right, and 0 is most anti EU, and 10 most pro EU). However, because some values are NA, Gephi has interpreted these as a series of characters or ‘Strings’, rather than a number. Looking back to our first week, the best way to visualise this information is treating the value as ‘continuous’, meaning, for example, we could colour it with a lighter or darker hue depending on whether the party was strongly pro or strongly anti-EU.\nTo do this, we need to specify to Gephi that it should intepret these columns as numbers. On this page, each of the columns in the data are listed, along with a drop-down menu where we can choose to change the type of data value Gephi will use. Most of the time the default is fine, but in this case, click on the drop-downs under left_right and eu_anti_pro, and change them to ‘double’:\n\nClick ‘Finish’, and you’ll get another screen. It will show some errors, which you can ignore. You should make one change here: change the option from ‘New workspace’ to ‘Append to existing workspace’:\n\nThe data should now load into Gephi and look like this:"
  },
  {
    "objectID": "network-analysis.html#copy-the-mp-names-to-the-label-column",
    "href": "network-analysis.html#copy-the-mp-names-to-the-label-column",
    "title": "9  Network Analysis",
    "section": "Copy the MP names to the label column",
    "text": "Copy the MP names to the label column\nBefore we move on, we need to make one further change to the node list. When visualising the network later, Gephi takes the node labels from the column called ‘Label’, which it has added automatically, and currently is empty. This should be filled with the ‘mp_name’ column in the data. To do this, click on the ‘copy data to other column’ button at the bottom of the screen, and select ‘mp_name’ from the list. A window will pop up asking you which column you want to copy the mp_name information to. If ‘label’ is not selected, select it here, and click OK."
  },
  {
    "objectID": "network-analysis.html#upload-the-edge-list-to-gephi",
    "href": "network-analysis.html#upload-the-edge-list-to-gephi",
    "title": "9  Network Analysis",
    "section": "Upload the edge list to gephi",
    "text": "Upload the edge list to gephi\nThe next step is to upload the edge list - the list of connected MPs based on co-sponsored bills. Because we have prepared the data correctly, Gephi will automatically notice that it should be connected to the names and information in the nodes list.\nAs before, click on ‘Import spreadsheet’. Open the ‘edge_list.csv’ file on the next screen and click OK. This time, you don’t need to make any changes to the import settings, so click ‘Next’, and then ‘Finish’.\nThis next step is very important. As before, on the next window, change from ‘New Workspace’ to ‘Append to existing workspace’, in order for the edge list to be linked to your node list. That’s it: you’ve successfully imported the network, and now you can visualise it!"
  },
  {
    "objectID": "network-analysis.html#the-overview-screen",
    "href": "network-analysis.html#the-overview-screen",
    "title": "9  Network Analysis",
    "section": "The Overview Screen",
    "text": "The Overview Screen\nTo work on visualising the network, click the Overview button to move to correct screen. You’ll see a series of connected points in the middle of the screen - this is the network with a random starting layour, and a pretty complicated looking set of options and buttons to choose from. At the top-left are a bunch of options for changing the size and colour of the nodes and edges. Underneath this are options to run a layout algorithm. On the right-hand side are some options to run statistics and find out more about your network.\n\nAs it is, the network diagram is not much use. The position of the nodes is at random, so it doesn’t tell us anything about the structure of this network, and the nodes are all the same size and colour. We can use gephi options to make it much more informative. We’ll start by selecting and adjusting a layout algorithm which most clearly shows the structure, and then move on to labelling and colouring the nodes."
  },
  {
    "objectID": "network-analysis.html#choosing-a-layout-algorithm",
    "href": "network-analysis.html#choosing-a-layout-algorithm",
    "title": "9  Network Analysis",
    "section": "Choosing a layout algorithm",
    "text": "Choosing a layout algorithm\nFirst, we’ll choose a layout algorithm. Layout algorithms generally try to place the nodes in a network in a way that reveals the structure of that network, usually using some kind of simulation of physical forces, such as gravity or springs. Gephi has a bunch of different ones to choose from. We’ll use ForceAtlas2, which often works well. Under the layout options, click on the drop-down menu and select ForceAtlas2 from the list.\n\nTo see what it looks like with the default settings, click ‘Run’. This will repeatedly run the algorithm until you tell it to stop. Usually, you’ll want to click stop when it stops changing in shape (though it might still be moving, for example spinning in circles). The result will look something like this, which isn’t very readable:"
  },
  {
    "objectID": "network-analysis.html#adjust-the-layout-algorithm-to-make-it-more-readable",
    "href": "network-analysis.html#adjust-the-layout-algorithm-to-make-it-more-readable",
    "title": "9  Network Analysis",
    "section": "Adjust the layout algorithm to make it more readable",
    "text": "Adjust the layout algorithm to make it more readable\nHowever, there are many options to choose from which can help to make it more readable. With your own network, the best thing to do is experiment and continually re-run the algorithm. Look for settings which allow you to determine differences in the shape and structure of the network, and make it less clustered and easier to read.\nFor this network, try the following settings.\n\nFirst, select ‘LinLog mode’. This will exaggerate the differences between the nodes, spreading them out and making the clusters easier to see.\nChange the ‘gravity’ setting from 1 to 3. At the default setting, unattached nodes (those who only co-sponsored themselves) disappeared. This brings them closer to the centre, so they are visible.\nClick the ‘prevent overlap’ button. This adjusts the nodes slightly so they do not overlap with each other.\nChange the ‘scaling’ option to 10, to further separate out the groups of nodes from each other.\n\nYou should now see something like this, which is not perfect but some structure does begin to emerge:"
  },
  {
    "objectID": "network-analysis.html#add-and-adjust-the-labels.",
    "href": "network-analysis.html#add-and-adjust-the-labels.",
    "title": "9  Network Analysis",
    "section": "Add and adjust the labels.",
    "text": "Add and adjust the labels.\nNext, we would like to display the names of each of the nodes (MPs). To do this, click on the following icon:\n\nYou will also see two sliders, to the right of this. The first one adjusts the thickness of the edges, and the second adjusts the size of the labels. Experiment with these until you get a useable size.\nMany of the labels are unreadable because they are overlapping with each other. Gephi has an option to fix this. Click again on the layout drop-down, and select ‘Label adjust’ from the list. Click run - it will adjust the labels so they don’t overlap, and stop automatically. Now the network should be somewhat readable:"
  },
  {
    "objectID": "network-analysis.html#color-the-nodes-by-party-politics-andor-party-name.",
    "href": "network-analysis.html#color-the-nodes-by-party-politics-andor-party-name.",
    "title": "9  Network Analysis",
    "section": "Color the nodes by party politics and/or party name.",
    "text": "Color the nodes by party politics and/or party name.\nAt this point, we can see that the network has some kind of structure. There is a large cluster of connected MPs, a few smaller clusters, and a few outliers. But what connects them together? Do they form along party and political lines? We can use Gephi to colour the nodes by their attributes from the nodes_list table. We’ll try two options: first, colour the nodes by their party, and second, try a continuous colour, based on their party’s place on the left-right spectrum.\n\nColour by party\nThe options to colour nodes are at the top-left of the screen. This contains four icons: the first gives options for colouring nodes and edges, the second for sizing them, the third for colouring the labels, and the fourth for sizing the labels. Click on the painter palette if it is not already selected, and then the ‘Partition’ option. Click the drop-down menu and select ‘mp_party’. This will display a colour scheme:\n\nClick apply, and the nodes will change colour.\n\n\nColour by party position\nWe also have data on the position on the left-right spectrum for the party of each MP (not the individual MP views, unfortunately). We can try colouring the nodes by this value too. Click on the ‘ranking’ option, and select ‘left_right’. Click apply to apply the default colour scheme, which runs from light green (for most left wing) to dark green (most right wing). However, if you think back to the lecture on colour theory, you’ll remember that in a case like this, it is better to use a diverging colour palette, where most left wing will be one dark colour, those in the middle will have light colour, and most right-wing will have a different dark colour. To change the palette, click on the small icon to the right of the colour bar:\n\nNotice that the palettes at the bottom consist of two colours with a light colour in the middle. Choose one of these, and apply it, which will colour the nodes by their party’s position:"
  },
  {
    "objectID": "network-analysis.html#calculate-and-look-at-network-statistics",
    "href": "network-analysis.html#calculate-and-look-at-network-statistics",
    "title": "9  Network Analysis",
    "section": "Calculate and look at network statistics",
    "text": "Calculate and look at network statistics\nWhile we’re more interested in the visual aspects for this tutorial, Gephi also allows you to calculate a series of network measurements, such as finding the most important nodes. As a demonstration, we’ll calculate the ‘betweenness centrality’ scores for the nodes in the network, and then use this value to re-size the nodes.\n\nCalculate scores\nTo do this, look to the ‘statistics’ tab on the right-hand side. The measurement we’re interested in is hidden under the ‘Network diameter’ option. Click on this, then in the next window, change the option from ‘directed’ to ‘undirected’ - as this particular network is undirected. Click OK, and you’ll get a series of graphs in a new window, which you can close.\n\n\nSize by betweenness\nNow, we can include this information in the network visualisation, by sizing the nodes according to their betweenness centrality score. Go back to the ‘appearance’ window in the top-left, and click on the size option, to the right of the palette. Next, click ‘ranking’, then choose betweenness centrality from the drop-down menu. Click apply. You can adjust the minimum and maximum sizes to make it easier to read, if you like.\n\nYou can experiment with other measurements - which can also be set to colour values."
  },
  {
    "objectID": "network-analysis.html#preview-and-export",
    "href": "network-analysis.html#preview-and-export",
    "title": "9  Network Analysis",
    "section": "Preview and export",
    "text": "Preview and export\nThe last step is to create and export a publication-ready copy of your network.\nThis is done using the ‘Preview’ screen. Move to this by clicking on the button at the top of the screen. Here you can select a bunch of options for how you want the final version to look. Use the dropdown to select ‘Default straight’ to begin with, to draw the default version with straight rather than curved edges. Click ‘refresh’, to get a preview of the final file. If you’re happy with it, click export and save it as a .pdf, to be uploaded for this week’s assignment.\n\nThe final image should look something like the below. Feel free to adjust the options and layout to make it as readable as possible. For example, try following the instructions here: https://stackoverflow.com/questions/38975945/labels-only-for-nodes-with-a-minimal-weight-of-x-in-Gephi to hide some of the smaller labels."
  },
  {
    "objectID": "network-analysis.html#create-a-network-using-vistorian.net",
    "href": "network-analysis.html#create-a-network-using-vistorian.net",
    "title": "9  Network Analysis",
    "section": "Create a network using Vistorian.net",
    "text": "Create a network using Vistorian.net\nIf you can’t install Gephi, here is an alternative method to create a network, using an online application called The Vistorian. It is not as customisable as Gephi, but can create nice interactive graphs which are particularly useful for data exploration.\nFirst, download these alternative versions of the files, in a slightly different format:\nEdge list\nNode list\nGo to https://vistorian.net/ in a browser, and click ’start my session.\nOn the next screen, click ‘Create network’, choose a name for it, and click Next.\n\nThe Vistorian will ask a series of questions about your data. Click the correct option in each case, then hit Next to move on.\n\nFor ‘What is the Format of your data?’, select\n\nI have one or several files in a tabular format (e.g., spreadsheet, CSV, ...)\n\nFor How are links (edges) represented in your network?. select\n\nLink table: A table containing one row per link. Each row contains a pair of nodes that are linked\n\nFor Are Links Directed?, select No\n\nClick on Upload a new file in the box, and upload the edge_list_v.csv file. Select the following options (see screenshot below):\n\nAdd a header row, by ticking the checkbox\nSet the source and target labels to mp_1 and mp_2\nSet the link weight to the weight column\n\n\nHit next. On the next page, you’ll be asked if you have an additional node file. Click yes, and upload the node_list_v.csv file. Click to select a header row, set the node ID to name, and the Node Type to party_name.\n\nHit next, and your data should be successfully loaded. You’ll see a number of options for network visualisations. Our data doesn’t have temporal or geographic data associated with it, so only the first two are relevant.\nStart with the Node Link visualisation, and experiment with node and link attributes in order to make a readable and useable graph. When you’re done, take a screenshot and upload to the assignment area on Brightspace. It should hopefully look something like this:"
  },
  {
    "objectID": "design.html#introduction",
    "href": "design.html#introduction",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Introduction",
    "text": "Introduction\nUp until now, we have concentrated on the techniques for making charts and maps from data sources. This week, we’ll concentrate on what I’m calling the ‘finishing touches’: the final parts of your charts which will make them more attractive, and most importantly, readable, to viewers.\nThese can be divided into two categories:\n\nElements ‘extra’ to the chart itself, such as titles (main titles, subtitles, titles on the axes), the background color, and guide lines.\nElements within the chart such as labels and annotations."
  },
  {
    "objectID": "design.html#chart-titles",
    "href": "design.html#chart-titles",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Chart titles",
    "text": "Chart titles\nThe easiest way to improve the readability of your chart is the add titles to help the reader. This is dependent on context, too. It’s often not required or recommended to add a chart title in an academic book or article, but a chart in a report or piece of data journalism should usually have a chart.\nA good principle is to try to make your chart stand on its own without further explanation, as far as possible. Even if your visualisation is embedded within a report, readers often skip to the chart first without looking at the underlying context. And charts often get separated from their context if they are shared on social media, for example.\nFor a chart to stand alone, consider adding the following:\n\nAn eye-catching title, which explains the ‘headline’ of your chart.\nA subtitle, with further explanation\nA caption, which gives other relevant information, particularly the data source, or any necessary caveats to the data.\n\nTitles (and labels) can be added to charts in ggplot by adding the code + labs(). Within + labs(), you can specify title =, subtitle =, and caption =, which are fairly self-explanatory. Make sure to include the text of your label within quotation marks. If you would like to spread your title across multiple lines, you can specify a line break by adding the code /n within your title.\nLet’s demonstrate how to make charts more readable and informative with an example.\nTake this basic chart which charts the number of asylum seekers by country of origin to Germany, using the UNCHR data and R package. I’ve made a chart which visualises as a line the count of refugees from each country of origin , for the years 2000 to 2020:\n\noptions(scipen = 999999) # this means large numbers will not be drawn as scientific notation\n\n# load the UNCHR data or package and tidyverse library\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload('population.rda')\n  \n\n# first, create a count by year and country of origin:\n\nall = population %&gt;% \n  filter(coa_iso == 'DEU') %&gt;% #filter to just Germany as country of arrival\n  group_by(year, coo) %&gt;% \n  summarise(n = sum(asylum_seekers)) %&gt;% # sum the total number of asylum seekers for each origin and year\n  filter(year %in% c(2000:2020)) # filter to only include the years 2000-2020\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n# next make the chart. \n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo)) # if we specify group as the country of origin, it will draw each as a separate line.\n\n\n\n\nFirst, let’s add a title, subtitle, and caption:\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo)) + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\")\n\n\n\n\nWe can also use labs() to specify new names for the x and y axes, or removing one by setting to NULL:\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo)) + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\",\n       x = NULL,\n       y = \"Number seeking asylum\")\n\n\n\n\n\nExercises\nCopy the chart into a new cell and add the following:\n\nAdd a subtitle with the text “2000-2020 inclusive”,\nAdd a caption with the text “Data from https://www.unhcr.org/refugee-statistics/”"
  },
  {
    "objectID": "design.html#extra-chart-elements-using-theme",
    "href": "design.html#extra-chart-elements-using-theme",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Extra-chart elements using theme()",
    "text": "Extra-chart elements using theme()\nUsing the default settings will make an accurate representation of your data but there are lots of ways to make a chart more readable.\nEach visual part of a chart is known as an element, and we can make changes to each part separately. This is done by adding +theme() plus some extra code as a layer to your chart.\nLet’s see how this works with an example, first. I want to change the size of the x and y axes text in the chart we made above\nTo do this, I first add a new layer with + theme() to my existing plot.\nNext, within this theme(), I add:\n\nThe plot element I would like to change, which in this case is axis.title followed by an = sign. We can also specify separate using axis.title.x or axis.title.y.\nThe type of element it is (we’ll come back to this), either element_text(), element_rect(), element_line() or, if I want to remove it entirely, element_blank().\n\nIn this case, it is a text element, so we use element_text().\nNext, within this element_text(), we specify the change we want to make. For a text element, we can change the size, the font (using family) and whether it is bold or italic (using face).\nTo change the size of the text to 16, use the following full line of code:\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo)) + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\nDifferent types of elements need to be specified in different ways, and have different aspects which can be adjusted. Below is a full list of the plot parts and their related element types:"
  },
  {
    "objectID": "design.html#exercises-1",
    "href": "design.html#exercises-1",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Exercises:",
    "text": "Exercises:\nCopy and paste the chart above into a new cell. Make the following changes:\n\nChange the panel background fill to lightblue.\nChange the size of the title to 24, and the ‘face’ to bold.\nChange the panel grid to the linetype ‘dashed’."
  },
  {
    "objectID": "design.html#drawing-readers-to-the-data-in-your-chart",
    "href": "design.html#drawing-readers-to-the-data-in-your-chart",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Drawing readers to the data in your chart",
    "text": "Drawing readers to the data in your chart\nThe chart above might be a faithful representation of the data, but it doesn’t really tell a story.\nTo do this, we’ll draw viewers’ attention to particular parts of the data using a few techniques. These use a cognitive process known as ‘pre-attentive processing’, meaning we are drawn to certain visual elements before others. Highlighting using color and annotations are ways to use this to our advantage.\n\nHighlighting using color\nOne effective method to draw attention to certain elements is to highlight using colours. This can be done fairly easily using our existing toolset. The set of steps are as follows:\n\nStart with your basic chart. Perhaps change colors or transparency to make the majority of the data ‘fade’ into the background.\nNext create new datasets containing only the data you wish to highlight.\nAdd these as layers to your existing chart\nChange the colors of these new elements\n\nFor instance, let’s highlight asylum seekers from Syria within the plot. We’ll add the data for Syria as a new geom_line(), and we’ll specify a larger size and a different color.\n\n# first all the data (we made this earlier but just to highlight)\n\nall = population %&gt;% \n  filter(coa_iso == 'DEU') %&gt;% \n  group_by(year, coo) %&gt;% \n  summarise(n = sum(asylum_seekers)) %&gt;% \n  filter(year %in% c(2000:2020)) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n# now a new dataset but with an extra filter\n\nsyria = population %&gt;% \n  filter(coa_iso == 'DEU' & coo_iso == 'SYR') %&gt;% \n  group_by(year, coo) %&gt;% \n  summarise(n = sum(asylum_seekers)) %&gt;% \n  filter(year %in% c(2000:2020))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n# create the plot\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) + #reduce the transparency of the other lines\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + # add a new line and specify size/color\n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nExercises:\n\nAdd a second line with a different color, this time for asylum seekers with a country of origin Afghanistan."
  },
  {
    "objectID": "design.html#adding-text-and-annotations",
    "href": "design.html#adding-text-and-annotations",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Adding text and annotations",
    "text": "Adding text and annotations\nAnother really useful technique for creating a narrative in your data visualisation is to add annotations. This is usually to draw attention to particular aspects of the data, or to explain or give context. The idea is to guide your reader towards the story in your data.\nThere are quite a few ways of adding text. The simplest way is to add an annotation with a layer called annotate(). This allows us to add text but also shapes or lines.\nTo create a text annotation, we need to specify a few things within annotate():\n\nThe type of annotation, in this case text.\nThe x and y position the annotation should be placed. This is done manually and usually requires a bit of experimentation.\nThe actual label to be displayed\nOptionally, we can specify color, size, face, family and so forth…\n\nLet’s add a label to show our readers that the highlighted line is Syria\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  annotate('text', x = 2014, y = 75000, label = 'Syria', color = 'forestgreen', size = 5) + # add the annotation\n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\n\nExercises\n\nAdd a similar label for Afghanistan. Find a suitable place on the chart where it will be readable\n\n\n\nAdding lines as annotations\nAnother useful technique for adding contextual information, particularly with line charts, is to add vertical lines highlighting particular points in time.\nThis is done using another element, called geom_vline(). In this case, we specify where the line should be placed using xintercept. Optionally, we can set the size, the linetype (e.g. to dashed), and the transparency using alpha.\nLet’s add a line to the chart to highlight an important milestone in the Syrian civil war: the beginning of revolts in March 2011. Note that because the data is not in date format but simply numbers, we have to add the line at 2011.25 to make it approximately March 2011.\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  annotate(geom = 'text', x = 2014, y = 75000, label = 'Syria', color = 'forestgreen', size = 5) + \n  geom_vline(xintercept = 2011.25, linetype = 'dashed') + # add the line, specifying the linetype. \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\nExercises:\n\nAdd a similar line for September 2015, when the German government announced that asylum seekers would be welcomed in Germany.\n\n\n\nText annotations and lines\nOn their own, these lines are not enough. We can also add labels to the line. For this, we return to the annotate() element.\nFirst, let’s draw a label in an empty area of the chart, using annotate().\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  annotate(geom = 'text', x = 2014, y = 75000, label = 'Syria', color = 'forestgreen', size = 5) + \n  geom_vline(xintercept = 2011.25, linetype = 'dashed') + \n  annotate(geom = 'text', x = 2007, y = 110000, label = \"Beginning of Syrian Revolt\") + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\nNext we can draw a curved line to connect the text label to the vertical line.\nAgain, we use annotate(). This time, the geom type is set to curve. When we make a curve, we need to specify the x beginning and end using x and xend, and the y beginning and end, using y and yend.\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  annotate(geom = 'text', x = 2014, y = 75000, label = 'Syria', color = 'forestgreen', size = 5) + \n  geom_vline(xintercept = 2011.25, linetype = 'dashed') + \n  annotate(geom = 'text', x = 2007, y = 110000, label = \"Beginning of Syrian Revolt\") + \n  annotate(geom = 'curve', x = 2007, y = 100000, xend = 2011, yend = 65000,\n    arrow = arrow(length = unit(0.3, 'cm'), type = 'closed')) + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\nOptionally, we can add an arrow to the end of the line. The syntax starts to get a bit complicated, so copy and paste is your friend!\nFirst, specify to draw an arrow by placing arrow =within the annotate(). Next, we use the following code to tell it what arrow to draw:\narrow = arrow(length = unit(.3, 'cm'), type = 'closed')\nWithin arrow(), we specify the length of the arrow, within unit() where we also specify what time of unit (e.g. cm, mm, in). Lastly, we can specify either a ‘closed’ or ‘open’ arrow type.\n\nggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  annotate(geom = 'text', x = 2014, y = 75000, label = 'Syria', color = 'forestgreen', size = 5) + \n  geom_vline(xintercept = 2011.25, linetype = 'dashed') + \n  annotate(geom = 'text', x = 2007, y = 110000, label = \"Beginning of Syrian Revolt\") + \n  annotate(geom = 'curve', x = 2007, y = 100000, xend = 2011, yend = 65000, # add the \n    arrow = arrow(length = unit(0.3, 'cm'), type = 'closed')) + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\n\n\n\n\n\nExercises\nFor both of these, use copy and paste where possible - start by copying in the identical code, and then make changes to the values as necessary.\n\nAdd a text label for the second highlighted milestone. Find a suitable place on the chart (it can also be to the right)\nCreate a connecting line from the label to the vertical line. Optionally, add an arrow."
  },
  {
    "objectID": "design.html#if-time-interactive-charts",
    "href": "design.html#if-time-interactive-charts",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "If time: interactive charts",
    "text": "If time: interactive charts\nAnother way to make more readable charts is to include interactive elements. This can help viewers to find interesting points in the data by hovering and clicking, or by zooming in. This negates the need for annotations and labels in many cases, meaning we can make less cluttered visualisations.\nThese interactive charts will display in the same .html documents we have used for the weekly exercises and assignments.\nIn R, we can use a library called plotly to easily turn our visualisations interactive.\nFirst, create a regular ggplot plot. Importantly, you should name this plot in your environment by setting a name followed by =.\nWe’ll remove some unnecessary elements, such as the labels and lines, but leave in the colored extra line and the titles.\n\np = ggplot() + \n    geom_line(data = all, aes(x = year, y = n, group = coo), alpha = .5) +\n  geom_line(data =syria, aes(x = year, y = n), size = 1.5, color = 'forestgreen') + \n  labs(title = \"Asylum seekers to Germany\\nby country of origin\", \n       subtitle = \"2000-2020 inclusive\", \n       caption = \"Data from https://www.unhcr.org/refugee-statistics/\",\n       x = NULL,\n       y = \"Number seeking asylum\") + \n  theme(axis.title = element_text(size = 16))\n\nNow, load the library plotly. If it’s not installed, install using `install.packages(‘plotly’).\nYou can make plots directly in plotly using this syntax, or you can use a function called ggplotly() to automatically convert your static chart. Simply pass your chart name to the function:\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.2.3\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nggplotly(p)"
  },
  {
    "objectID": "design.html#interactive-maps-with-tmap",
    "href": "design.html#interactive-maps-with-tmap",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Interactive maps with tmap",
    "text": "Interactive maps with tmap\nAnother feature to improve your data visualisations is to use interactive maps. Again, there are several ways to do this, including the package Leaflet.\nThe easiest way to create a map is with a package called tmap. This takes an sf object (similar to those we created in earlier weeks), and allows you to make a nice map using a syntax fairly similar to ggplot. Optionally, you can make these maps interactive.\nTo demonstrate this, I’ll use a dataset from the Shakespeare & Co project at Princeton, which contains information on the locations of members of the famous Shakespeare & Co bookshop and lending library in Paris.\nFirst, install and load the tmap package.\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n\nLoad the dataset:\n\nsco_data = read_csv('SCoData_members_v1.2_2022_01.csv')\n\nRows: 5235 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (14): uri, name, sort_name, title, gender, membership_years, viaf_url, ...\ndbl   (2): birth_year, death_year\nlgl   (2): is_organization, has_card\ndttm  (1): updated\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsco_data = sco_data %&gt;% separate(coordinates, into = c('latitude', 'longitude'), sep = ',')\n\nWarning: Expected 2 pieces. Additional pieces discarded in 255 rows [23, 25, 72, 97,\n100, 118, 197, 263, 309, 329, 342, 358, 384, 401, 404, 405, 411, 412, 426, 461,\n...].\n\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [1080, 1572,\n2307].\n\n\nNow, create an sf object, as we learned in previous weeks\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.2.3\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nsco_data_sf = st_as_sf(sco_data, coords = c('longitude', 'latitude'), na.fail = FALSE)\n\nWarning in lapply(x[coords], as.numeric): NAs introduced by coercion\n\n\nWarning in lapply(x[coords], as.numeric): NAs introduced by coercion\n\n\nTo specify an interactive map with tmap use tmap_mode(\"view\")\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\nBuild the map. First add a ‘basemap’, which is a zoomable world map. A full list of options is available here.\nNow, add the data using tm_shape(), and specify how it should be drawn using tm_dots(). The map below is interactive and clickable:\n\ntm_basemap('OpenStreetMap.Mapnik') + \n  tm_shape(sco_data_sf) + \n  tm_dots()\n\nWarning: Currect projection of shape sco_data_sf unknown. Long-lat (WGS84) is\nassumed.\n\n\nWarning: The shape sco_data_sf contains empty units."
  },
  {
    "objectID": "design.html#homework",
    "href": "design.html#homework",
    "title": "10  Finishing Touches: Creating Narratives with Your Datavis",
    "section": "Homework",
    "text": "Homework\n\nWrite up a plan for your final project\nWhere will you get the data?\nWhat story do you want to tell?\nWe’ll discuss next week."
  },
  {
    "objectID": "final-project.html#instructions",
    "href": "final-project.html#instructions",
    "title": "11  Final Project",
    "section": "Instructions",
    "text": "Instructions\n\nFor the final project, you should create a ‘data story’ with a dataset of your choice.\nYour data story should be a publication-quality article aimed at a general interested reader, for example in an academic journal, a ‘long read’ article in a magazine, or a report for an NGO or non-profit. . It should weave in text, visualisations, and the underlying code.\nIt should contain at least four separate visualisations. However, two of these should be unique types, and the other two can be simpler derivatives (e.g. focusing on a certain range through filter, or perhaps highlighting different parts of the visualisation through annotation or color highlighting.\nThe text should be approximately 1000 - 1500 words in total.\nYou can use any dataset you like - there are many examples and links to further datasets to be found throughout the course book."
  },
  {
    "objectID": "final-project.html#to-get-a-high-grade-consider-the-following",
    "href": "final-project.html#to-get-a-high-grade-consider-the-following",
    "title": "11  Final Project",
    "section": "To get a high grade, consider the following:",
    "text": "To get a high grade, consider the following:\n\nThe visualisations should be interspersed with your story, e.g. the background, the data (what you had to do to it), then a narrative of the process for creating the visualisations, findings, and a conclusion.\nThe report as a whole should have a strong, cohesive narrative (don’t just make a bunch of disconnected graphs and call it a day).\nThe publication as a whole should be reproducible. By this I mean, I should be able to run your code and get the same results. You should include the code and datasets used in your final submission.\nThe visualisations, as far as possible, should be self-contained. If someone shared on social media for example, they would stand along without explanation and not be mis-interpreted. Use titles, subtitles, annotations, and captions. Optionally, make your visualisations interactive - but explain how a viewer should use it within your report.\nThe whole report should be of publication quality. Make effective use of the markdown techniques we learned early in the course, such as headers, bullet points, and even inserting additional images. Ideally, hide the code output from the html version (we’ll do this next week)."
  },
  {
    "objectID": "final-project.html#submission",
    "href": "final-project.html#submission",
    "title": "11  Final Project",
    "section": "Submission",
    "text": "Submission\nAs with the other assignments, submit a .zip folder containing the following:\n\nThe code, in the form of an .Rmd file.\nThe final publication, as an .html\nAny datasets needed to reproduce the code."
  },
  {
    "objectID": "final-project.html#deadline",
    "href": "final-project.html#deadline",
    "title": "11  Final Project",
    "section": "Deadline",
    "text": "Deadline\nThe final deadline for this is 14 January 2024.\nAs I realise that some aspects can be unnecessarily tricky (e.g. the markdown formatting), I invite you to submit a draft version a week earlier, on 7 January 2024, which I will comment on, mostly in terms of technical issues."
  },
  {
    "objectID": "assignment-1.html",
    "href": "assignment-1.html",
    "title": "12  Assignment 1: Visualize This!",
    "section": "",
    "text": "Deadline: 20-11-2021 (Before 9.00AM)\nFor the first assignment, create a static visualisation based on the topic and datasets you identified in week 2.\n\nYou can either use some data you have found and we have discussed in class, or one of the datasets on the Datasets page. The Federal Gifts Register is a good option if you don’t have a strong preference for your own story at this stage.\nAdding other sources is allowed, but not mandatory. \nVisualize the story in a static visualization using a graph that compares categories, shows a ratio part-to-whole, or shows relationship.\n\nUse the fantastic The Graphic Continuum to see which types of graphs you can use for that.\n\nShow some basic evidence of data wrangling, e.g. filtering, summarising or joining the data.\nHand in via Brighstpace a .zip file with:\n\nA html report made using R markdown, containing the code (including data importing and processing/wrangling steps, the visualisation, plus a short report (500 - 1000 words) which covers:\n\nThe context and motivation behind the visualisation\nThe visualization’s story you are trying to convey \nListing and (briefly) discussing all your design choices pertinent to that story.\nAny challenges or opportunities met along the way (if any), to indicate your learning process\n\nThe source code (.Rmd file) for the report.\nThe dataset used (you’ll need to upload this to Posit cloud, then export it along with the other two items. Ask if you’re not sure!\n\nThe assignment should take you about 10 hours."
  },
  {
    "objectID": "assignment-2.html#goal",
    "href": "assignment-2.html#goal",
    "title": "13  Assignment 2: Make a map",
    "section": "Goal",
    "text": "Goal\nFor this assignment, make either a choropleth or points map. This map should visualise some aspect of your chosen dataset, using aesthetics such as color, size and shape (depending on your map type).\nThe map should be readable and you should make design choices to improve it where possible.\nFor this assignment you will hand in, in one zip file via Brightspace:\n\nA visualization, in the form of a markdown report, containing the final map and code\nA report (500 words max) providing information on the map’s design decisions made, referring to Chapter 4 of Meirelles’ Design for Information and Chapter 15 of Claus Wilke’s Data Visualization where appropriate. You will conclude the report by mentioning any challenges encountered (if any) and providing a project timetable.\nYou should return both the .Rmd file and the HTML file in your .zip file."
  },
  {
    "objectID": "assignment-2.html#deadline",
    "href": "assignment-2.html#deadline",
    "title": "13  Assignment 2: Make a map",
    "section": "Deadline",
    "text": "Deadline\nThe deadline to hand up this assignment is 11 December, at 09.00."
  },
  {
    "objectID": "assignment-2.html#tips",
    "href": "assignment-2.html#tips",
    "title": "13  Assignment 2: Make a map",
    "section": "Tips",
    "text": "Tips\nLook back over the previous weeks to find relevant code to complete the exercise. Pay particular attention to the method for joining your dataset up to a basemap from RNaturalEarth, if you’re making a choropleth map."
  },
  {
    "objectID": "assignment-2.html#datasets",
    "href": "assignment-2.html#datasets",
    "title": "13  Assignment 2: Make a map",
    "section": "Datasets",
    "text": "Datasets\nYou are welcome to use any spatial dataset you like, but here are some options if you don’t have something in mind:\n\nThe Presidential Gifts dataset\nThe Nobel Prize dataset (the code to download this can be found in the course book, for example in Digital Mapping 3).\nPeople map of the UK (explanation here)\nICE facilities in the US (from the Torn Apart/Separados project)\nHistorical letter data. To use this, you would need to join the ‘Letters’ or ‘People’ data to the ‘Cities’ dataset, in order to join coordinates to data.\nThe ‘members’ dataset from theShakespeare and Co project, which lists the addresses of members of the famous bookshop and lending library in Paris. If you’d like to use this, you’ll probably need a basemap which is at the ‘zoom’ level of a single city (Paris). Get in touch and I can help with this!\nYou can also make your own dataset, using a geocoder, a gazetteer, or looking up coordinates manually."
  }
]